{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c271658",
   "metadata": {},
   "source": [
    "# LLM is a great rule-based feature engineer in few-shot tabular learning\n",
    "## Overview\n",
    "This notebook runs training and inference for few-shot tabular learning task over benchmark datasets. GPT-3.5 model is used in this tutorial.\n",
    "\n",
    "## Overall process\n",
    "* Prepare datasets\n",
    "* Extract rules for prediction from training samples with the help of LLM\n",
    "* Parse rules to the program code and convert data into the binary vector\n",
    "* Train the linear model to predict the likelihood of each class from the binary vector\n",
    "* Make inference with ensembling\n",
    "\n",
    "## Differences in V6\n",
    "* New pipeline\n",
    "\n",
    "**Notes**\n",
    "There are two major flaws within our logs and experiments. Logs with the outdated system may have versioning that leaves out details and includes versioning that isn't correct. For those, there was no distinction between condition prompting and interaction conditions prompting having different versioning for I only varied the interaction condition prmopting. Thus, anything with a -v2 is just cXv0 and icXv3 and -v1 is probably xCv0 and icXv2. There are probabyl are experiments with icXv0 but those are minimal. \n",
    "\n",
    "Second, I mislabeled the prompts as numerical for the longest time until August 4th. All experiments with the proper labeling in the feature description will now have a dash at the end with the note that clarifies this.\n",
    "\n",
    "\n",
    "**DISCLAIMER:**\n",
    "This code is heavily based on the open-source project FeatLLM developed by Sungwon Han.\n",
    "You can find the original repository at: https://github.com/Sungwon-Han/FeatLLM.\n",
    "\n",
    "Many of the functions and methodologies implemented here are derived from or inspired by the work in FeatLLM.\n",
    "All credit goes to the original authors for their invaluable contributions to this project.\n",
    "Modifications and extensions to the original codebase have been made to tailor it to specific use cases and requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "df4835c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils)\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5ba82",
   "metadata": {},
   "source": [
    "## Prepare datasets\n",
    "1. Set dataset and simulation parameters (e.g. # of training shots, and the random seed)\n",
    "2. Get SNPs data and split it into train/test dataset, given simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "955b2a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_NUM_QUERY = 20 # Number of Queries\n",
    "_SHOT = 16 # Number of training shots (cannot be 15-shot)\n",
    "_SEED = 3 # Seed for fixing randomness\n",
    "_NUM_OF_CONDITIONS = 15\n",
    "_NUM_OF_CONDITIONS_FOR_INTERACTIONS = 0\n",
    "_DATA = \"hearing_loss_pyramid_llm_select_modified\"\n",
    "#_MODEL = \"gpt-4o-2024-05-13\"\n",
    "_MODEL = \"gpt-3.5-turbo\"\n",
    "#_MODEL = \"Meta-Llama-3.1-405B-Instruct-Turbo\"\n",
    "_FUNCTION_MODEL = \"gpt-3.5-turbo\"\n",
    "_REWRITING_FUNCTION_MODEL = \"gpt-4-1106-preview\"\n",
    "_CONDITION_PROMPT_VERSION = \"v5\"\n",
    "_INTERACTION_PROMPT_VERSION = \"v0\"\n",
    "_NOTE = \"\" # Start note with a dash\n",
    "#_NOTE = \"-no-examples-only\"\n",
    "#_NOTE = \"-fixed-categorical-conditions\"\n",
    "_RECORD_LOGS = True\n",
    "_METADATA_VERSION = \"v0\" # Select higher versions for augmentation\n",
    "utils.set_seed(_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a6f62",
   "metadata": {},
   "source": [
    "now, let's get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f9558b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, X_train, X_test, y_train, y_test, target_attr, label_list, is_cat = utils.get_dataset(_DATA, _SHOT, _SEED)\n",
    "X_all = df.drop(target_attr, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db90de3d",
   "metadata": {},
   "source": [
    "## Extract rules for prediction from training samples with the help of LLM\n",
    "To enable the LLM to extract rules based on a more accurate reasoning path, we guided the problem-solving process to mimic how a person might approach a tabular learning task.   \n",
    "\n",
    "We divided the problem into two sub-tasks for this purpose:   \n",
    "1. Understand the task description and the features provided by the data, inferring the causal relationships beforehand.   \n",
    "2. Use the inferred information and few-shot samples to deduce the prediction rules for each class. This two-step reasoning process prevents the model from identifying spurious correlations in irrelevant columns and assists in focusing on more significant features.   \n",
    "\n",
    "Our prompt comprises three main components as follows:  \n",
    "* Task description\n",
    "* Reasoning instruction\n",
    "* Response instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "27c63e6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c.35delG': '', 'c.235delC': '', 'p.V37I': '', 'A1555G': '', 'p.W77X': '', 'p.R75W': '', 'p.E47X': '', 'p.R143Q': '', 'p.R143W': '', 'c.234_235delC': '', 'p.R75Q': '', 'p.V207L': '', 'c.919_2A>G': '', 'p.G316X': '', 'p.E303Q': ''}\n",
      "You are an expert in genetics. Given the task description and the list of features and data examples, you are extracting and engineering novel features to solve the task. The purpose of this process is to generate a set of rich, dense and robust features that better express the data.\n",
      "\n",
      "## Task\n",
      "Does the subject have hereditary hearing loss? With regards to SNP variants, no mutations being found for the SNP are indicated by 0, heterozygous mutations by 1, and homozygous mutations by 2.\n",
      "\n",
      "\n",
      "## Features\n",
      "- c.35delG\n",
      "- c.235delC\n",
      "- p.V37I\n",
      "- A1555G\n",
      "- p.W77X\n",
      "- p.R75W\n",
      "- p.E47X\n",
      "- p.R143Q\n",
      "- p.R143W\n",
      "- c.234_235delC\n",
      "- p.R75Q\n",
      "- p.V207L\n",
      "- c.919_2A>G\n",
      "- p.G316X\n",
      "- p.E303Q\n",
      "\n",
      "## Examples\n",
      "c.35delG is 0. c.235delC is 0. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 1. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: No, does not have hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 0. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: No, does not have hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 0. p.V37I is 1. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: No, does not have hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 0. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: No, does not have hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 0. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: No, does not have hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 0. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: No, does not have hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 0. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: No, does not have hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 1. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: No, does not have hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 0. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: Yes, has hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 0. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 1. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: Yes, has hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 2. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: Yes, has hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 0. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: Yes, has hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 2. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: Yes, has hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 0. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: Yes, has hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 0. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: Yes, has hereditary hearing loss\n",
      "\n",
      "c.35delG is 0. c.235delC is 2. p.V37I is 0. A1555G is 0. p.W77X is 0. p.R75W is 0. p.E47X is 0. p.R143Q is 0. p.R143W is 0. c.234_235delC is 0. p.R75Q is 0. p.V207L is 0. c.919_2A>G is 0. p.G316X is 0. p.E303Q is 0.\n",
      "Answer: Yes, has hereditary hearing loss\n",
      "\n",
      "\n",
      "\n",
      "## Detailed Instructions\n",
      "Based on the above examples and your extensive knowledge, engineer a few features with some of the features listed above. Consider the following actions for feature engineering for each [Feature_name]:\n",
      "- [Feature_name] is in [list of Categorical_values]\n",
      "- [Feature_name] (> or >= or < or <=) [Numerical_value]\n",
      "- [Feature1_name] + [Feature2_name] if you believe there is an additive effect\n",
      "- [Feature1_name] * [Feature2_name] if you believe there is a multiplicative effect \n",
      "- [Condition with Feature 1] AND [Condition with Feature 2] if you believe there's a tree-like interaction \n",
      "- Feel free to engineer other complex, creative interactions between features\n",
      "\n",
      "Here's a demonstration of how one might do feature engineering.\n",
      "\n",
      "GJB2 Mutation (c.35delG): The GJB2 gene encodes the Connexin 26 protein, which is crucial for cell-to-cell communication in the cochlea of the inner ear. The c.35delG mutation leads to a nonfunctional protein, disrupting this communication and causing hearing loss.\n",
      "SLC26A4 Mutation (c.919-2A>G): The SLC26A4 gene encodes the Pendrin protein, which is involved in ion transport in the inner ear. The c.919-2A>G mutation leads to improper splicing and a dysfunctional protein, contributing to hearing loss.\n",
      "Analysis of Synergistic Effect: Each mutation can cause hearing loss on its own. However, when both mutations are present in an individual, the risk and severity of hearing loss are significantly increased. The combined effect of these mutations disrupts multiple pathways critical for auditory function. The GJB2 mutation affects gap junction communication, while the SLC26A4 mutation impairs ion transport. Together, these disruptions can lead to more profound hearing loss compared to having a mutation in only one of these genes.\n",
      "Output: combined_count_c.35delG_c.919-2A>G = c.35delG + c.919-2A>G_count\n",
      "\n",
      "## Step-by-Step\n",
      "Let's first understand the problem and solve the problem step by step, with your own knowledge and without coding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(utils)\n",
    "if 'ancestry' in _DATA:\n",
    "    _DATA_TYPE = 'ancestry'\n",
    "else:\n",
    "    _DATA_TYPE = 'hearing_loss'\n",
    "if \"gpt\" in _MODEL:\n",
    "    ask_file_name = f'./templates/ask_llm_{_CONDITION_PROMPT_VERSION}_{_DATA_TYPE}.txt'\n",
    "else: \n",
    "    ask_file_name = f'./templates/ask_llm_llama_{_CONDITION_PROMPT_VERSION}.txt'\n",
    "meta_data_name = f\"../data/{_DATA}-metadata-{_METADATA_VERSION}.json\"\n",
    "templates, feature_desc = utils.get_prompt_for_asking(\n",
    "    _DATA, X_all, X_train, y_train, label_list, target_attr, ask_file_name, \n",
    "    meta_data_name, is_cat, num_query=_NUM_QUERY, num_conditions=_NUM_OF_CONDITIONS,\n",
    "    prompt_version = _CONDITION_PROMPT_VERSION\n",
    ")\n",
    "print(templates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ee074a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Understand the problem**: \n",
      "   - The task is to determine if a subject has hereditary hearing loss based on their genetic mutations.\n",
      "   - The provided data includes various SNP variants and their mutation status (0, 1, 2).\n",
      "\n",
      "2. **Feature Engineering**:\n",
      "   - **c.35delG** and **p.V37I**: Since both are mutations associated with Connexin 26 protein, we can create an interaction feature to capture the combined effect of these mutations: `combined_c.35delG_p.V37I = c.35delG * p.V37I`\n",
      "   - **p.R143Q** and **p.R143W**: These are two mutations at the same position. We can create a feature to capture the presence of either mutation: `either_p.R143Q_p.R143W = p.R143Q OR p.R143W`\n",
      "\n",
      "3. **Analysis**:\n",
      "   - The interaction feature `combined_c.35delG_p.V37I` can provide insights into how these mutations together affect the likelihood of hereditary hearing loss.\n",
      "   - The feature `either_p.R143Q_p.R143W` can help identify subjects with mutations at the specific position regardless of the type of mutation.\n",
      "\n",
      "4. **Final Steps**:\n",
      "   - Use the engineered features in your model to see if they improve the predictive power for determining hereditary hearing loss.\n",
      "   - Monitor the importance of these features in the model to understand their contribution to the prediction.\n"
     ]
    }
   ],
   "source": [
    "_DIVIDER = \"\\n\\n---DIVIDER---\\n\\n\"\n",
    "_VERSION = \"\\n\\n---VERSION---\\n\\n\"\n",
    "\n",
    "rule_file_name = f'./rules/{_DATA}/{_SHOT}_shot/rule-s{_SHOT}-c{_NUM_OF_CONDITIONS}{_CONDITION_PROMPT_VERSION}-{_MODEL}-q{_NUM_QUERY}-{_SEED}{_NOTE}.out'\n",
    "if os.path.isfile(rule_file_name) == False:\n",
    "    results = utils.query_gpt(templates, max_tokens=2000, temperature=1, model = _MODEL)\n",
    "    if _RECORD_LOGS:\n",
    "        with open(rule_file_name, 'w') as f:\n",
    "            total_rules = _DIVIDER.join(results)\n",
    "            f.write(total_rules)\n",
    "else:\n",
    "    with open(rule_file_name, 'r') as f:\n",
    "        total_rules_str = f.read().strip()\n",
    "        results = total_rules_str.split(_DIVIDER)\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "86263ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.92s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.54s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.09s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. combined_c.35delG_p.V37I = c.35delG * p.V37I\\n2. either_p.R143Q_p.R143W = p.R143Q OR p.R143W', '1. Combined mutations of SNP variants known to impact hearing loss\\n2. Patterns or combinations of mutations commonly found together in individuals with hereditary hearing loss\\n3. Presence of specific combinations of mutations known to increase the risk of hearing loss\\n4. Additive effects of mutations on hearing loss likelihood\\n5. Multiplicative effects of mutations on hearing loss likelihood', '1. Review the provided examples to understand the format of the data and the features that are relevant to hereditary hearing loss.\\n2. Analyze the significance of each SNP variant in relation to hereditary hearing loss based on existing literature and research.\\n3. Identify potential interactions between different SNP variants that may have a synergistic effect on hereditary hearing loss.\\n4. Consider the impact of homozygous mutations versus heterozygous mutations for each SNP variant on the risk of developing hereditary hearing loss.\\n5. Explore the possibility of combining multiple SNP variants to create new features that capture complex interactions and correlations.\\n6. Evaluate the overall genetic profile of an individual by aggregating information from multiple SNP variants to make a prediction about the likelihood of hereditary hearing loss.\\n7. Develop a comprehensive set of engineered features that encapsulate the key genetic factors contributing to hereditary hearing loss based on the provided SNP variants.\\n8. Consider the potential for adding additional external data sources or features to enhance the predictive power of the model.\\n9. Test and validate the engineered features using a suitable machine learning algorithm to assess their effectiveness in predicting hereditary hearing loss accurately.\\n10. Refine and iterate on the feature engineering process to optimize the predictive performance and interpretability of the model.', '1. List of features related to SNP variants\\n2. Contribution of each mutation to hereditary hearing loss\\n3. Potential interactions between mutations increasing risk/severity of hearing loss\\n4. Impact of multiple mutations vs. single mutation on likelihood of hearing loss\\n5. Features capturing interactions between mutations or indicating presence of multiple mutations\\n6. Complex features using logical operators (AND, OR, NOT) to express combined effects of mutations\\n7. Ensuring engineered features are informative, relevant, and enhance predictive power for detecting hereditary hearing loss', '1. Combination of mutations in SNP variants associated with hereditary hearing loss\\n2. Interaction between specific SNP variants contributing to hereditary hearing loss\\n3. New feature based on combined mutations in multiple SNP variants\\n4. Relevant engineered features for predicting hereditary hearing loss\\n5. Incorporation of engineered features into predictive model for classification', '1. Understand the significance of each SNP variant listed in the features. Research on how each mutation affects the hearing process and the genes involved.\\n2. Identify potential additive or multiplicative effects between SNP variants.\\n3. Look for interactions between specific mutations that may lead to a higher likelihood of hereditary hearing loss.\\n4. Consider the individual impact of each mutation as well as the combined effect when multiple mutations are present.\\n5. Explore any existing literature or studies on the interactions between different SNP variants related to hereditary hearing loss.\\n6. Engineer new features that capture the complex relationships between SNP variants and their impact on the likelihood of hereditary hearing loss.\\n7. Test the newly engineered features on the dataset to see if they improve the prediction accuracy or provide a better understanding of the genetic factors influencing hereditary hearing loss.', '1. Understand the role of each genetic variant in hearing loss\\n2. Identify potential interactions between genetic variants\\n3. Consider the mode of inheritance\\n4. Evaluate the functional consequences of each mutation\\n5. Investigate genetic modifiers\\n6. Analyze population frequency data\\n7. Generate novel features\\n8. Validate the engineered features\\n9. Iterate and refine', '1. Understand the genetic variations listed and their impact on hereditary hearing loss.\\n2. Identify potential interactions between different mutations that may lead to a higher risk of hearing loss.\\n3. Look for patterns in the examples provided to determine what combination of mutations indicates hereditary hearing loss.\\n4. Consider the impact of individual mutations versus the combined effect of multiple mutations on the risk of hearing loss.\\n5. Engineer features that capture the interactions between mutations and their impact on the likelihood of hereditary hearing loss.', '1. GJB2 gene with c.35delG mutation\\n2. SLC26A4 gene with c.919-2A>G mutation\\n3. c.35delG mutation in GJB2 disrupts Connexin 26 protein leading to hearing loss\\n4. c.919-2A>G mutation in SLC26A4 affects Pendrin protein causing hearing loss\\n5. Presence of both mutations can lead to increased risk and severity of hearing loss\\n6. Combined disruptions in gap junction communication and ion transport pathways can worsen hearing loss\\n7. Create a new feature `combined_mutations` to capture the interaction between c.35delG and c.919-2A>G mutations\\n8. Look for additive or multiplicative effects between other features to capture potential interactions\\n9. Consider creating features based on specific combinations of mutations that are known to have a synergistic effect in causing hereditary hearing loss\\n10. Ensure the engineered features capture the complexity and richness of genetic interactions related to hereditary hearing loss\\n11. Validate the new features with existing data to ensure they provide meaningful insights into the prediction task', '1. Analyze each SNP variant and its association with hereditary hearing loss\\n2. Identify SNP variants with strong relationship to hereditary hearing loss\\n3. Look for interactions between SNP variants\\n4. Combine SNP variants to create new features\\n5. Consider additive or multiplicative effects between SNP variants\\n6. Explore disrupted genetic pathways by SNP variant combinations\\n7. Create new engineered features based on insights from steps 2-6', '1. Review the features provided and their implications on hereditary hearing loss.\\n2. Consider the potential additive or multiplicative effects of certain mutations on the risk of hereditary hearing loss.\\n3. Look for interactions between different mutations that may lead to a higher risk of hearing loss.\\n4. Take into account the known biological pathways and functions of the genes associated with the mutations.\\n5. Determine if certain combinations of mutations have a greater impact on the likelihood of hereditary hearing loss.\\n6. Integrate this information into the feature engineering process to create new, more informative features that capture the complexity of genetic factors contributing to hereditary hearing loss.', '1. Combined effect of c.35delG and c.235delC mutations\\n2. Combined effect of p.R143Q and p.R75Q mutations\\n3. Joint impact of c.234_235delC and c.919_2A>G mutations\\n4. Interaction between p.V37I and p.R75W mutations\\n5. Interplay between A1555G and p.W77X mutations', '1. c.35delG_and_c.235delC_mutations = c.35delG + c.235delC\\n2. A1555G_times_p.W77X = A1555G * p.W77X\\n3. R75W_AND_R75Q_mutations = p.R75W AND p.R75Q\\n4. c.234_235delC_times_c.919_2A>G = c.234_235delC * c.919_2A>G\\n5. R143Q_OR_R143W_mutations = p.R143Q OR p.R143W', '1. Review the list of features provided and their significance in hereditary hearing loss.\\n2. Identify any known interactions or additive effects between certain mutations that may contribute to a higher likelihood of hereditary hearing loss.\\n3. Consider the individual impact of each mutation on the risk of hereditary hearing loss.\\n4. Explore potential genetic pathways or mechanisms that could be affected by the combination of certain mutations.\\n5. Look for any patterns or commonalities among individuals with hereditary hearing loss in terms of the mutations present.\\n6. Consider the rarity or prevalence of certain mutations in individuals with hereditary hearing loss.\\n7. Think about any potential gene-gene interactions or epistasis effects that may influence the likelihood of hereditary hearing loss.', '1. Combine mutations in GJB2 gene (c.35delG) and SLC26A4 gene (c.919-2A>G) to assess cumulative effect on hearing loss risk.\\n2. Engineer interactions between different SNP variants to understand synergistic effects on hearing loss risk.\\n3. Identify patterns in SNP variants common among subjects with hereditary hearing loss and engineer features based on those patterns.\\n4. Consider impact of homozygous mutations versus heterozygous mutations on hearing loss risk and engineer features accordingly.', '- combined_mut_c.35delG_c.235delC = c.35delG + c.235delC\\n- combined_mut_p.V37I_p.V207L = p.V37I * p.V207L\\n- combined_mut_A1555G_p.G316X = A1555G + p.G316X', '1. Mutations in the GJB2 gene (e.g. c.35delG and c.235delC) associated with non-syndromic hearing loss\\n2. Synergistic effects of certain combinations of mutations on the risk of hearing loss\\n3. Inheritance patterns of mutations (autosomal dominant or recessive)\\n4. Biological functions of mutated genes contributing to hearing loss (e.g. ion transport, cell-to-cell communication in the inner ear)\\n5. New engineered features capturing the complexity of genetic variants and their combined effects on the risk of developing hearing loss', '1. Examine impact of each SNP variant on hereditary hearing loss\\n2. Identify relationships between SNP variants\\n3. Consider genotype of each SNP variant as a feature\\n4. Explore additive and multiplicative effects of SNP variants\\n5. Investigate synergistic effects of SNP variant combinations\\n6. Consider other genetic factors and environmental influences\\n7. Create new engineered features based on relationships and interactions', '1. c.35delG_c.235delC_interaction = c.35delG + c.235delC\\n2. A1555G_G316X_interaction = A1555G * G316X\\n3. p.V37I_p.R143Q_interaction = p.V37I + p.R143Q\\n4. total_mutation_status = sum of all mutation statuses (0, 1, 2)', '1. c.35delG: Mutation in the GJB2 gene affecting Connexin 26 protein\\n2. c.235delC: Another mutation associated with hearing loss\\n3. p.V37I, p.W77X, p.R75W, p.E47X, p.R143Q, p.R143W, p.R75Q, p.V207L, p.G316X, p.E303Q: Various mutations with potential impact on hearing loss\\n4. A1555G: Mutation associated with aminoglycoside-induced hearing loss\\n5. Investigate if certain combinations of mutations lead to a higher likelihood of hereditary hearing loss\\n6. Check if specific mutations, when present together, have a synergistic effect on hearing loss\\n7. Create a feature that represents the total count of mutations related to hearing loss\\n8. Consider interactions like multiplication or addition between specific mutations to capture their combined effect\\n9. Explore if certain mutations together have a higher likelihood of causing hereditary hearing loss compared to individual mutations\\n10. Analyze the engineered features to see if they provide better insights into the likelihood of hereditary hearing loss based on the genetic mutations present\\n11. Continuously refine and optimize the features based on new data and insights to improve the predictive power of the model for identifying hereditary hearing loss.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_rules = []\n",
    "\n",
    "# Iterate through each result in the results list\n",
    "for result in results:\n",
    "    # Use utils.query_gpt to transform each result\n",
    "    transformed_result = utils.query_gpt(\n",
    "        [f\"Extract the list of engineered features (include their equation or instructions) and list them one after another in a new line: {result}\\n\\nIf some features are clumped up together, make sure to list them separately.\\n\\nList:\"], \n",
    "        max_tokens=2000, \n",
    "        temperature=0, \n",
    "        model=_FUNCTION_MODEL\n",
    "    )\n",
    "    # Append the transformed result to the results_transformed list\n",
    "    parsed_rules.append(transformed_result[0])\n",
    "\n",
    "# The parsed_rules list now contains all the transformed results\n",
    "print(parsed_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e14d006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Understand the problem**: \n",
      "   - The task is to determine if a subject has hereditary hearing loss based on their genetic mutations.\n",
      "   - The provided data includes various SNP variants and their mutation status (0, 1, 2).\n",
      "\n",
      "2. **Feature Engineering**:\n",
      "   - **c.35delG** and **p.V37I**: Since both are mutations associated with Connexin 26 protein, we can create an interaction feature to capture the combined effect of these mutations: `combined_c.35delG_p.V37I = c.35delG * p.V37I`\n",
      "   - **p.R143Q** and **p.R143W**: These are two mutations at the same position. We can create a feature to capture the presence of either mutation: `either_p.R143Q_p.R143W = p.R143Q OR p.R143W`\n",
      "\n",
      "3. **Analysis**:\n",
      "   - The interaction feature `combined_c.35delG_p.V37I` can provide insights into how these mutations together affect the likelihood of hereditary hearing loss.\n",
      "   - The feature `either_p.R143Q_p.R143W` can help identify subjects with mutations at the specific position regardless of the type of mutation.\n",
      "\n",
      "4. **Final Steps**:\n",
      "   - Use the engineered features in your model to see if they improve the predictive power for determining hereditary hearing loss.\n",
      "   - Monitor the importance of these features in the model to understand their contribution to the prediction.\n"
     ]
    }
   ],
   "source": [
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e00f3eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. c.35delG_and_c.235delC_mutations = c.35delG + c.235delC\n",
      "2. A1555G_times_p.W77X = A1555G * p.W77X\n",
      "3. R75W_AND_R75Q_mutations = p.R75W AND p.R75Q\n",
      "4. c.234_235delC_times_c.919_2A>G = c.234_235delC * c.919_2A>G\n",
      "5. R143Q_OR_R143W_mutations = p.R143Q OR p.R143W\n"
     ]
    }
   ],
   "source": [
    "print(parsed_rules[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a316dc4",
   "metadata": {},
   "source": [
    "## Parse rules to the program code and convert data into the binary vector\n",
    "\n",
    "We utilize the rules generated in the previous stage to transform each sample into a binary vector. These vectors are created for each answer class, indicating whether the sample satisfies the rules associated with that class. However, since the rules generated by the LLM are based on natural language, parsing the text into program code is required for automatic data transformation.  \n",
    "\n",
    "To address the challenges of parsing noisy text, instead of building complex program code, we leverage the LLM itself. We include the function name, input and output descriptions, and inferred rules in the prompt, then input it into the LLM. The generated code is executed using Python’s exec() function along with the provided function name to perform data conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "69c7e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "_DIVIDER = \"\\n\\n---DIVIDER---\\n\\n\"\n",
    "_VERSION = \"\\n\\n---VERSION---\\n\\n\"\n",
    "\n",
    "saved_file_name = f'./rules/{_DATA}/{_SHOT}_shot/function-s{_SHOT}-c{_NUM_OF_CONDITIONS}{_CONDITION_PROMPT_VERSION}-ic{_NUM_OF_CONDITIONS_FOR_INTERACTIONS}{_INTERACTION_PROMPT_VERSION}-{_MODEL}-{_FUNCTION_MODEL}-q{_NUM_QUERY}-{_SEED}{_NOTE}.out'    \n",
    "if os.path.isfile(saved_file_name) == False:\n",
    "    function_file_name = './templates/ask_for_function_v2.txt'\n",
    "    fct_strs_all = []\n",
    "    for parsed_rule in tqdm(parsed_rules):\n",
    "        fct_template = utils.get_prompt_for_generating_function_simple(\n",
    "            parsed_rule, feature_desc, function_file_name\n",
    "        )\n",
    "        fct_results = utils.query_gpt(fct_template, max_tokens=2500, temperature=0, model = _FUNCTION_MODEL)\n",
    "        fct_strs = [fct_txt.split('<start>')[1].split('<end>')[0].strip() for fct_txt in fct_results]\n",
    "        fct_strs_all.append(fct_strs[0])\n",
    "    if _RECORD_LOGS:\n",
    "        with open(saved_file_name, 'w') as f:\n",
    "            total_str = _VERSION.join([x for x in fct_strs_all])\n",
    "            f.write(total_str)\n",
    "else:\n",
    "    with open(saved_file_name, 'r') as f:\n",
    "        total_str = f.read().strip()\n",
    "        fct_strs_all = [x.split(_DIVIDER) for x in total_str.split(_VERSION)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6104e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'e', 'f', ' ', 'e', 'x', 't', 'r', 'a', 'c', 't', 'i', 'n', 'g', '_', 'e', 'n', 'g', 'i', 'n', 'e', 'e', 'r', 'e', 'd', '_', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', '(', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', ')', ':', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', ' ', '=', ' ', 'p', 'd', '.', 'D', 'a', 't', 'a', 'F', 'r', 'a', 'm', 'e', '(', ')', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'c', '.', '3', '5', 'd', 'e', 'l', 'G', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'c', '.', '3', '5', 'd', 'e', 'l', 'G', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'c', '.', '2', '3', '5', 'd', 'e', 'l', 'C', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'c', '.', '2', '3', '5', 'd', 'e', 'l', 'C', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'p', '.', 'V', '3', '7', 'I', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'p', '.', 'V', '3', '7', 'I', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'A', '1', '5', '5', '5', 'G', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'A', '1', '5', '5', '5', 'G', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'p', '.', 'W', '7', '7', 'X', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'p', '.', 'W', '7', '7', 'X', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'p', '.', 'R', '7', '5', 'W', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'p', '.', 'R', '7', '5', 'W', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'p', '.', 'E', '4', '7', 'X', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'p', '.', 'E', '4', '7', 'X', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'p', '.', 'R', '1', '4', '3', 'Q', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'p', '.', 'R', '1', '4', '3', 'Q', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'p', '.', 'R', '1', '4', '3', 'W', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'p', '.', 'R', '1', '4', '3', 'W', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'c', '.', '2', '3', '4', '_', '2', '3', '5', 'd', 'e', 'l', 'C', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'c', '.', '2', '3', '4', '_', '2', '3', '5', 'd', 'e', 'l', 'C', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'p', '.', 'R', '7', '5', 'Q', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'p', '.', 'R', '7', '5', 'Q', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'p', '.', 'V', '2', '0', '7', 'L', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'p', '.', 'V', '2', '0', '7', 'L', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'c', '.', '9', '1', '9', '_', '2', 'A', '>', 'G', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'c', '.', '9', '1', '9', '_', '2', 'A', '>', 'G', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'p', '.', 'G', '3', '1', '6', 'X', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'p', '.', 'G', '3', '1', '6', 'X', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'p', '.', 'E', '3', '0', '3', 'Q', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'p', '.', 'E', '3', '0', '3', 'Q', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'N', 'E', 'W', '_', 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '1', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '1', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'N', 'E', 'W', '_', 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '2', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '2', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'N', 'E', 'W', '_', 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '3', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '3', \"'\", ']', '.', 'a', 'p', 'p', 'l', 'y', '(', 'l', 'a', 'm', 'b', 'd', 'a', ' ', 'x', ':', ' ', '1', ' ', 'i', 'f', ' ', 'x', ' ', 'i', 'n', ' ', '[', '1', ',', '2', ']', ' ', 'e', 'l', 's', 'e', ' ', '0', ')', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'N', 'E', 'W', '_', 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '4', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '4', \"'\", ']', '.', 'a', 'p', 'p', 'l', 'y', '(', 'l', 'a', 'm', 'b', 'd', 'a', ' ', 'x', ':', ' ', '1', ' ', 'i', 'f', ' ', 'x', ' ', 'i', 'n', ' ', '[', '0', ',', '1', ']', ' ', 'e', 'l', 's', 'e', ' ', '0', ')', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'N', 'E', 'W', '_', 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '5', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '.', 'a', 'p', 'p', 'l', 'y', '(', 'l', 'a', 'm', 'b', 'd', 'a', ' ', 'r', 'o', 'w', ':', ' ', '1', ' ', 'i', 'f', ' ', 'r', 'o', 'w', '[', \"'\", 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '3', \"'\", ']', ' ', '>', '=', ' ', '0', ' ', 'a', 'n', 'd', ' ', 'r', 'o', 'w', '[', \"'\", 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '9', \"'\", ']', ' ', '>', '=', ' ', '1', ' ', 'e', 'l', 's', 'e', ' ', '0', ',', ' ', 'a', 'x', 'i', 's', '=', '1', ')', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'N', 'E', 'W', '_', 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '6', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '.', 'a', 'p', 'p', 'l', 'y', '(', 'l', 'a', 'm', 'b', 'd', 'a', ' ', 'r', 'o', 'w', ':', ' ', '1', ' ', 'i', 'f', ' ', 'r', 'o', 'w', '[', \"'\", 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '4', \"'\", ']', ' ', '=', '=', ' ', '1', ' ', 'a', 'n', 'd', ' ', 'r', 'o', 'w', '[', \"'\", 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '8', \"'\", ']', ' ', '=', '=', ' ', '0', ' ', 'e', 'l', 's', 'e', ' ', '0', ',', ' ', 'a', 'x', 'i', 's', '=', '1', ')', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'N', 'E', 'W', '_', 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '7', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '.', 'a', 'p', 'p', 'l', 'y', '(', 'l', 'a', 'm', 'b', 'd', 'a', ' ', 'r', 'o', 'w', ':', ' ', 'r', 'o', 'w', '[', \"'\", 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '4', \"'\", ']', ' ', '*', ' ', 'r', 'o', 'w', '[', \"'\", 'F', 'E', 'A', 'T', 'U', 'R', 'E', '_', '5', \"'\", ']', ',', ' ', 'a', 'x', 'i', 's', '=', '1', ')', '\\n', ' ', ' ', ' ', ' ', '\\n', ' ', ' ', ' ', ' ', 'r', 'e', 't', 'u', 'r', 'n', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't']\n"
     ]
    }
   ],
   "source": [
    "# Examine function output\n",
    "print(fct_strs_all[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "111da968",
   "metadata": {},
   "outputs": [],
   "source": [
    "critique_fct_strs_all = fct_strs_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08cd8ce",
   "metadata": {},
   "source": [
    "#### Self-Critiqueing Function Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3cc801bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'list' object has no attribute 'strip'\n",
      "Function string to critique: ['d', 'e', 'f', ' ', 'e', 'x', 't', 'r', 'a', 'c', 't', 'i', 'n', 'g', '_', 'e', 'n', 'g', 'i', 'n', 'e', 'e', 'r', 'e', 'd', '_', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', '(', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', ')', ':', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', ' ', '=', ' ', 'p', 'd', '.', 'D', 'a', 't', 'a', 'F', 'r', 'a', 'm', 'e', '(', ')', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'c', 'o', 'm', 'b', 'i', 'n', 'e', 'd', '_', 'c', '.', '3', '5', 'd', 'e', 'l', 'G', '_', 'p', '.', 'V', '3', '7', 'I', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'c', '.', '3', '5', 'd', 'e', 'l', 'G', \"'\", ']', ' ', '*', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', \"'\", 'p', '.', 'V', '3', '7', 'I', \"'\", ']', '\\n', ' ', ' ', ' ', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't', '[', \"'\", 'e', 'i', 't', 'h', 'e', 'r', '_', 'p', '.', 'R', '1', '4', '3', 'Q', '_', 'p', '.', 'R', '1', '4', '3', 'W', \"'\", ']', ' ', '=', ' ', 'd', 'f', '_', 'i', 'n', 'p', 'u', 't', '[', '[', \"'\", 'p', '.', 'R', '1', '4', '3', 'Q', \"'\", ',', ' ', \"'\", 'p', '.', 'R', '1', '4', '3', 'W', \"'\", ']', ']', '.', 'm', 'a', 'x', '(', 'a', 'x', 'i', 's', '=', '1', ')', '\\n', ' ', ' ', ' ', ' ', '\\n', ' ', ' ', ' ', ' ', 'r', 'e', 't', 'u', 'r', 'n', ' ', 'd', 'f', '_', 'o', 'u', 't', 'p', 'u', 't']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m reload(utils)\n\u001b[0;32m----> 2\u001b[0m critique_fct_strs_all \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_critique_functions_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed_rules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfct_strs_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_NUM_OF_CONDITIONS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_NUM_OF_CONDITIONS_FOR_INTERACTIONS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_REWRITING_FUNCTION_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_tolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/research/AD_RAG/src/geno_RAG/src/notebooks/utils.py:678\u001b[0m, in \u001b[0;36mself_critique_functions_simple\u001b[0;34m(parsed_rules, feature_desc, fct_strs_all, X_train, _NUM_OF_CONDITIONS, _NUM_OF_CONDITIONS_FOR_INTERACTIONS, _REWRITING_FUNCTION_MODEL, condition_tolerance)\u001b[0m\n\u001b[1;32m    676\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mThe function is supposed to follow these instructions:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstructions_for_query_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHowever, the following function was flagged for the following error: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m! Fix this error and output just the fixed function. Wrap only the function part with <start> and <end>, and do not add any comments, descriptions, and package importing lines in the code. Here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the function you need to fix:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;66;03m# print(context)\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m critique_results \u001b[38;5;241m=\u001b[39m query_gpt([\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfct_strs\u001b[49m], max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2500\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model\u001b[38;5;241m=\u001b[39m_REWRITING_FUNCTION_MODEL)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;66;03m# Extract critiqued function strings\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;66;03m# critique_fct_strs = [fct_txt.split('<start>')[1].split('<end>')[0].strip() for fct_txt in critique_results]\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# print(critique_results[0])\u001b[39;00m\n\u001b[1;32m    682\u001b[0m critique_fct_strs_all\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(critique_results))\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<start>\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<end>\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip())\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "reload(utils)\n",
    "critique_fct_strs_all = utils.self_critique_functions_simple(parsed_rules, feature_desc, fct_strs_all, X_train, _NUM_OF_CONDITIONS, _NUM_OF_CONDITIONS_FOR_INTERACTIONS, _REWRITING_FUNCTION_MODEL, condition_tolerance=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a2690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _RECORD_LOGS:\n",
    "    with open(saved_file_name, 'w') as f:\n",
    "        total_str = _VERSION.join([_DIVIDER.join(x) for x in critique_fct_strs_all])\n",
    "        f.write(total_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get function names and strings\n",
    "fct_names = []\n",
    "fct_strs_final = []\n",
    "for fct_str in critique_fct_strs_all:\n",
    "    if 'def' not in fct_str:\n",
    "        continue\n",
    "    fct_names.append(fct_str.split('def')[1].split('(')[0].strip())\n",
    "    fct_strs_final.append(fct_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520dcea",
   "metadata": {},
   "source": [
    "### Checking Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a5980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def extracting_features(df_input):\n",
      "    df_output = pd.DataFrame()\n",
      "    df_output['NEW_FEATURE_1'] = df_input['FEATURE_1']\n",
      "    df_output['NEW_FEATURE_2'] = df_input['FEATURE_2']\n",
      "    df_output['NEW_FEATURE_3'] = df_input['FEATURE_3'].apply(lambda x: 1 if x in [1,2] else 0)\n",
      "    df_output['NEW_FEATURE_4'] = df_input['FEATURE_4'].apply(lambda x: 1 if x in [0,1] else 0)\n",
      "    df_output['NEW_FEATURE_5'] = df_input.apply(lambda row: 1 if row['FEATURE_3'] >= 0 and row['FEATURE_9'] >= 1 else 0, axis=1)\n",
      "    df_output['NEW_FEATURE_6'] = df_input.apply(lambda row: 1 if row['FEATURE_4'] == 1 and row['FEATURE_8'] == 0 else 0, axis=1)\n",
      "    df_output['NEW_FEATURE_7'] = df_input.apply(lambda row: row['FEATURE_4'] * row['FEATURE_5'], axis=1)\n",
      "    \n",
      "    return df_output\n"
     ]
    }
   ],
   "source": [
    "# Print an arbitrary but particular function\n",
    "print(critique_fct_strs_all[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f99c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined_c.35delG_p.V37I</th>\n",
       "      <th>either_p.R143Q_p.R143W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     combined_c.35delG_p.V37I  either_p.R143Q_p.R143W\n",
       "874                         0                     0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check some arbitrary function if it works\n",
    "exec(critique_fct_strs_all[0].strip('` \"'))\n",
    "fct_strs_all[0]\n",
    "locals()[fct_names[0]](X_train.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585fbb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined_c.35delG_p.V37I</th>\n",
       "      <th>either_p.R143Q_p.R143W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   combined_c.35delG_p.V37I  either_p.R143Q_p.R143W\n",
       "0                         0                       0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check some arbitrary function if it works on the test data\n",
    "exec(critique_fct_strs_all[0].strip('` \"'))\n",
    "fct_strs_all[0]\n",
    "locals()[fct_names[0]](X_test.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04263e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(critique_fct_strs_all[1].strip('` \"'))\n",
    "#locals()[fct_names[1][1]](X_test.head(268)).astype('int').to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c96e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'extracting_engineered_features'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fct_names[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c39c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def extracting_engineered_features(df_input):\n",
      "    df_output = pd.DataFrame()\n",
      "    df_output['GJB2_gene_with_c.35delG_mutation'] = df_input['c.35delG']\n",
      "    df_output['SLC26A4_gene_with_c.919_2A>G_mutation'] = df_input['c.919_2A>G']\n",
      "    df_output['c.35delG_mutation_in_GJB2_disrupts_Connexin_26_protein'] = df_input['c.35delG'].apply(lambda x: 1 if x == 1 else 0)\n",
      "    df_output['c.919_2A>G_mutation_in_SLC26A4_affects_Pendrin_protein'] = df_input['c.919_2A>G'].apply(lambda x: 1 if x == 1 else 0)\n",
      "    df_output['Presence_of_both_mutations_can_lead_to_increased_risk_and_severity_of_hearing_loss'] = df_input.apply(lambda row: 1 if row['c.35delG'] == 1 and row['c.919_2A>G'] == 1 else 0, axis=1)\n",
      "    df_output['Combined_disruptions_in_gap_junction_communication_and_ion_transport_pathways_can_worsen_hearing_loss'] = df_input.apply(lambda row: 1 if row['c.35delG'] == 1 or row['c.919_2A>G'] == 1 else 0, axis=1)\n",
      "    df_output['combined_mutations'] = df_input['c.35delG'] * df_input['c.919_2A>G']\n",
      "    \n",
      "    return df_output\n"
     ]
    }
   ],
   "source": [
    "# This is the final list of functions that generates the features for each ensemble\n",
    "# Type: str[][]\n",
    "print(critique_fct_strs_all[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c1e34",
   "metadata": {},
   "source": [
    "### Convert to binary vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d8141",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = X_test.notna().all(axis=1)\n",
    "\n",
    "# Dropping weird NAs\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb55be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, Error in convert_to_binary_vectors: 'FEATURE_1'\n",
      "                    Is the # of columns in X_train equal to X_test after applying the functions? True\n",
      "                    Is the # of rows the same after applying the functions? True\n",
      "                    How many conditional features after applying the functions? 15\n",
      "Iteration 4, Error in convert_to_binary_vectors: 'FEATURE_1'\n",
      "                    Is the # of columns in X_train equal to X_test after applying the functions? True\n",
      "                    Is the # of rows the same after applying the functions? True\n",
      "                    How many conditional features after applying the functions? 15\n",
      "Iteration 5, Error in convert_to_binary_vectors: 'FEATURE_1'\n",
      "                    Is the # of columns in X_train equal to X_test after applying the functions? True\n",
      "                    Is the # of rows the same after applying the functions? True\n",
      "                    How many conditional features after applying the functions? 15\n",
      "Iteration 17, Error in convert_to_binary_vectors: 'FEATURE_1'\n",
      "                    Is the # of columns in X_train equal to X_test after applying the functions? True\n",
      "                    Is the # of rows the same after applying the functions? True\n",
      "                    How many conditional features after applying the functions? 15\n"
     ]
    }
   ],
   "source": [
    "reload(utils)\n",
    "executable_list, X_train_all_dict, X_test_all_dict = utils.convert_to_binary_vectors_simple(fct_strs_final, \n",
    "                                                                                     fct_names, \n",
    "                                                                                     X_train, \n",
    "                                                                                     X_test, \n",
    "                                                                                     num_of_features=_NUM_OF_CONDITIONS+_NUM_OF_CONDITIONS_FOR_INTERACTIONS,\n",
    "                                                                                     include_original_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d781a3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of functions should be == # of ensembles. If not, some of the functions are faulty and broke when applying to training data.\n",
    "len(executable_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea38b2b",
   "metadata": {},
   "source": [
    "## Train the linear model to predict the likelihood of each class from the binary vector\n",
    "When given the rules for each class and a sample, a simple method to measure the class likelihood of the sample is to count how many rules of each class it satisfies (i.e., the sum of the binary vector per class). However, not all rules carry the same importance, necessitating learning their significance from training samples.    \n",
    "  \n",
    "We aimed to train this importance using a basic linear model without bias, applied to each class's binary vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbdcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.5837500862247361,\n",
       " 'Accuracy': 0.335676625659051,\n",
       " 'F1-Score': 0.3000211316316901,\n",
       " 'Class No Precision': 0.24742268041237114,\n",
       " 'Class No Recall': 0.9022556390977443,\n",
       " 'Class No F1-Score': 0.3883495145631068,\n",
       " 'Class Yes Precision': 0.8452380952380952,\n",
       " 'Class Yes Recall': 0.1628440366972477,\n",
       " 'Class Yes F1-Score': 0.27307692307692305}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "multiclass = True if len(label_list) > 2 else False\n",
    "y_train_num = np.array([label_list.index(k) for k in y_train])\n",
    "y_test_num = np.array([label_list.index(k) for k in y_test])\n",
    "mlp = RandomForestClassifier()\n",
    "\n",
    "# Fit the model\n",
    "mlp.fit(X_train, y_train_num)\n",
    "lr_pred_probs_train = mlp.predict_proba(X_train)\n",
    "lr_metrics_train = utils.evaluate(lr_pred_probs_train, y_train_num, multiclass=multiclass, class_level_analysis=True, label_list=label_list)\n",
    "lr_pred_probs_test = mlp.predict_proba(X_test)\n",
    "lr_metrics_test = utils.evaluate(lr_pred_probs_test, y_test_num, multiclass=multiclass, class_level_analysis=True, label_list=label_list)\n",
    "lr_metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170073d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of features:  17\n",
      "0.5837500862247361\n",
      "num of features:  30\n",
      "0.5643150306960061\n",
      "num of features:  30\n",
      "0.5827671242325998\n",
      "num of features:  30\n",
      "0.5615558391391323\n",
      "num of features:  30\n",
      "0.5616075739808236\n",
      "num of features:  22\n",
      "0.5593657308408636\n",
      "num of features:  30\n",
      "0.5827153893909085\n",
      "num of features:  30\n",
      "0.5827153893909085\n",
      "num of features:  20\n",
      "0.5832413602814375\n",
      "num of features:  20\n",
      "0.5610471131958337\n",
      "num of features:  30\n",
      "0.560572877146996\n",
      "num of features:  25\n",
      "0.5614609919293648\n",
      "num of features:  18\n",
      "0.561098848037525\n",
      "num of features:  30\n",
      "0.5837673311719667\n",
      "num of features:  19\n",
      "0.5924329171552735\n",
      "num of features:  30\n",
      "0.5632803338621784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.5805770159343313,\n",
       " 'Accuracy': 0.335676625659051,\n",
       " 'F1-Score': 0.3000211316316901,\n",
       " 'Class No Precision': 0.24742268041237114,\n",
       " 'Class No Recall': 0.9022556390977443,\n",
       " 'Class No F1-Score': 0.3883495145631068,\n",
       " 'Class Yes Precision': 0.8452380952380952,\n",
       " 'Class Yes Recall': 0.1628440366972477,\n",
       " 'Class Yes F1-Score': 0.27307692307692305}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "mlp_models = []\n",
    "\n",
    "# Train an MLP model on each version of the training data\n",
    "for X_train_now, X_test_now in zip(X_train_all_dict.values(), X_test_all_dict.values()):\n",
    "    mlp = RandomForestClassifier()\n",
    "    mlp.fit(X_train_now, y_train_num)\n",
    "    mlp_models.append(mlp)\n",
    "    lr_pred_probs_train = mlp.predict_proba(X_train_now)\n",
    "    lr_metrics_train = utils.evaluate(lr_pred_probs_train, y_train_num, multiclass=multiclass, class_level_analysis=True, label_list=label_list)\n",
    "    lr_pred_probs_test = mlp.predict_proba(X_test_now)\n",
    "    lr_metrics_test = utils.evaluate(lr_pred_probs_test, y_test_num, multiclass=multiclass, class_level_analysis=True, label_list=label_list)\n",
    "    print(\"num of features: \", X_train_now.shape[1])\n",
    "    print(lr_metrics_test['AUC'])\n",
    "\n",
    "# Initialize arrays to store ensemble predictions\n",
    "ensemble_pred_probs_train = np.zeros((X_train_all_dict[0].shape[0], len(label_list)))\n",
    "ensemble_pred_probs_test = np.zeros((X_test_all_dict[0].shape[0], len(label_list)))\n",
    "\n",
    "# Predict probabilities for training and test sets using each model and combine them\n",
    "for i, (X_train_now, X_test_now) in enumerate(zip(X_train_all_dict.values(), X_test_all_dict.values())):\n",
    "    ensemble_pred_probs_train += mlp_models[i].predict_proba(X_train_now)\n",
    "    ensemble_pred_probs_test += mlp_models[i].predict_proba(X_test_now)\n",
    "    \n",
    "\n",
    "# Average the probabilities\n",
    "ensemble_pred_probs_train /= len(X_train_all_dict)\n",
    "ensemble_pred_probs_test /= len(X_test_all_dict)\n",
    "\n",
    "# Evaluate the ensemble predictions\n",
    "ensemble_metrics_train = utils.evaluate(\n",
    "    ensemble_pred_probs_train, \n",
    "    y_train_num, \n",
    "    multiclass=multiclass, \n",
    "    class_level_analysis=True, \n",
    "    label_list=label_list\n",
    ")\n",
    "\n",
    "ensemble_metrics_test = utils.evaluate(\n",
    "    ensemble_pred_probs_test, \n",
    "    y_test_num, \n",
    "    multiclass=multiclass, \n",
    "    class_level_analysis=True, \n",
    "    label_list=label_list\n",
    ")\n",
    "\n",
    "# Output the test metrics\n",
    "ensemble_metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a7b8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tree.pdf'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "estimator = mlp.estimators_[4]\n",
    "\n",
    "# Export the tree to Graphviz format\n",
    "dot_data = export_graphviz(estimator, out_file=None,\n",
    "                           class_names=label_list,\n",
    "                           filled=True, rounded=True,\n",
    "                           special_characters=True)\n",
    "\n",
    "# Visualize the tree using graphviz\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(\"tree\")  # Save the tree as a file\n",
    "graph.view()  # View the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207affd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19594749 0.80405251]\n",
      " [0.19594749 0.80405251]\n",
      " [0.85450577 0.14549423]\n",
      " ...\n",
      " [0.55842546 0.44157454]\n",
      " [0.84578913 0.15421087]\n",
      " [0.55842546 0.44157454]]\n"
     ]
    }
   ],
   "source": [
    "print(ensemble_pred_probs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f775c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 1: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 3: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 6: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 7: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 8: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "          1., 0., 1., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0.]]),\n",
       " 9: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 10: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 11: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "          0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.]]),\n",
       " 12: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.]]),\n",
       " 13: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 14: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1.]]),\n",
       " 15: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 16: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 18: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,\n",
       "          2.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,\n",
       "          2.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,\n",
       "          2.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0.]]),\n",
       " 19: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a032f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 1: tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 3: tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 6: tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 7: tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 8: tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 9: tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 10: tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 11: tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 12: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 13: tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 14: tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]]),\n",
       " 15: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 16: tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 18: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 19: tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dacb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "------------------------------------------------------\n",
      "AUC                                          : 0.5806\n",
      "Accuracy                                     : 0.3357\n",
      "------------------------------------------------------\n",
      "F1-Score                                     : 0.3000\n",
      "Class No Precision                           : 0.2474\n",
      "Class No Recall                              : 0.9023\n",
      "------------------------------------------------------\n",
      "Class No F1-Score                            : 0.3883\n",
      "Class Yes Precision                          : 0.8452\n",
      "Class Yes Recall                             : 0.1628\n",
      "------------------------------------------------------\n",
      "Class Yes F1-Score                           : 0.2731\n"
     ]
    }
   ],
   "source": [
    "metrics = utils.evaluate(ensemble_pred_probs_test, y_test_num, multiclass=multiclass, class_level_analysis=True, label_list = label_list)\n",
    "# Function to print metrics in a readable format\n",
    "def print_metrics(metrics):\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "    i = 1\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric:45}: {value:.4f}\")\n",
    "        i += 1\n",
    "        if i % 3 == 0:\n",
    "            print(\"------------------------------------------------------\")\n",
    "        \n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dbe23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_file_name = f'./logs/{_DATA}/{_SHOT}_shot/evaluation-s{_SHOT}-c{_NUM_OF_CONDITIONS}{_CONDITION_PROMPT_VERSION}-ic{_NUM_OF_CONDITIONS_FOR_INTERACTIONS}{_INTERACTION_PROMPT_VERSION}-{_MODEL}-{_FUNCTION_MODEL}-{_NUM_QUERY}-{(len(executable_list))}-{_SEED}{_NOTE}.out'\n",
    "if _RECORD_LOGS:\n",
    "    with open(saved_file_name, 'w') as f:\n",
    "        f.write(\"Evaluation Metrics:\\n\")\n",
    "        f.write(\"------------------------------------------------------\\n\")\n",
    "        i = 1\n",
    "        for metric, value in metrics.items():\n",
    "            f.write(f\"{metric:45}: {value:.4f}\\n\")\n",
    "            i += 1\n",
    "            if i % 3 == 0:\n",
    "                f.write(\"------------------------------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinfo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
