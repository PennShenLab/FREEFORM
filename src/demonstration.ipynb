{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c271658",
   "metadata": {},
   "source": [
    "# LLM is a great rule-based feature engineer in few-shot tabular learning\n",
    "## Overview\n",
    "This notebook runs training and inference for few-shot tabular learning task over benchmark datasets. GPT-4o and GPT-3.5-turbo model is used in this tutorial.\n",
    "\n",
    "## Overall process\n",
    "* Feature selection\n",
    "* Prepare datasets\n",
    "* Extract rules for prediction from training samples with the help of LLM\n",
    "* Parse rules to the program code and convert data into the transformed datasets with rules\n",
    "* Train models with each of the datasets\n",
    "* Make inference with ensembling\n",
    "\n",
    "**DISCLAIMER:**\n",
    "This code is inspired from the open-source project FeatLLM developed by Sungwon Han.\n",
    "You can find the original repository at: https://github.com/Sungwon-Han/FeatLLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "df4835c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils)\n",
    "from tqdm import tqdm\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5ba82",
   "metadata": {},
   "source": [
    "## Prepare datasets\n",
    "1. Set dataset and simulation parameters (e.g. # of training shots, and the random seed)\n",
    "2. Get SNPs data and split it into train/test dataset, given simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "955b2a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_NUM_QUERY = 20 # Number of Queries\n",
    "_SHOT = 20 # Number of training shots \n",
    "_SEED = 0 # Seed for fixing randomness\n",
    "_NUM_OF_CONDITIONS = 15\n",
    "_NUM_OF_CONDITIONS_FOR_INTERACTIONS = 0\n",
    "_DATA = \"ancestry_15_features\"\n",
    "_MODEL = \"gpt-4o-2024-05-13\"\n",
    "_FUNCTION_MODEL = \"gpt-3.5-turbo\"\n",
    "_REWRITING_FUNCTION_MODEL = \"gpt-4-1106-preview\"\n",
    "_PROMPT_VERSION = \"v0\"\n",
    "_NOTE = \"\" # Start note with a dash\n",
    "_RECORD_LOGS = True\n",
    "_METADATA_VERSION = \"v0\" \n",
    "utils.set_seed(_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a6f62",
   "metadata": {},
   "source": [
    "now, let's get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f9558b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, X_train, X_test, y_train, y_test, target_attr, label_list, is_cat = utils.get_dataset(_DATA, _SHOT, _SEED)\n",
    "X_all = df.drop(target_attr, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db90de3d",
   "metadata": {},
   "source": [
    "## Extract rules for prediction from training samples with the help of LLM\n",
    "To enable the LLM to extract rules based on a more accurate reasoning path, we guided the problem-solving process to mimic how a person might approach a tabular learning task.   \n",
    "\n",
    "We divided the problem into two sub-tasks for this purpose:   \n",
    "1. Understand the task description and the features provided by the data, inferring the causal relationships beforehand.   \n",
    "2. Use the inferred information and few-shot samples to deduce the prediction rules for each class. This two-step reasoning process prevents the model from identifying spurious correlations in irrelevant columns and assists in focusing on more significant features.   \n",
    "\n",
    "Our prompt comprises three main components as follows:  \n",
    "* Task description\n",
    "* Reasoning instruction\n",
    "* Response instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "27c63e6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "You are an expert in genetics. Given the task description and the list of features and data examples, you are extracting and engineering novel features to solve the task. The purpose of this process is to generate a set of rich, dense and robust features that better express the data.\n",
      "\n",
      "## Task\n",
      "What is the subject's genomic ancestry? European, South Asian, East Asian, African, or American?\n",
      "\n",
      "\n",
      "## Features\n",
      "- rs671:  (numerical variable with categories [0,1,2])\n",
      "- rs1426654:  (numerical variable with categories [0,1,2])\n",
      "- rs16891982:  (numerical variable with categories [0,1,2])\n",
      "- rs4988235:  (numerical variable with categories [0,1,2])\n",
      "- rs12913832:  (numerical variable with categories [0,1,2])\n",
      "- rs2814778:  (numerical variable with categories [0,1,2])\n",
      "- rs1042602:  (numerical variable with categories [0,1,2])\n",
      "- rs10498746:  (numerical variable with categories [0,1,2])\n",
      "- rs3827760:  (numerical variable with categories [0,1,2])\n",
      "- rs2192416:  (numerical variable with categories [0,1,2])\n",
      "- rs1390723:  (numerical variable with categories [0,1,2])\n",
      "- rs3814381:  (numerical variable with categories [0,1,2])\n",
      "- rs2476746:  (numerical variable with categories [0,1,2])\n",
      "- rs1878685:  (numerical variable with categories [0,1,2])\n",
      "- rs1344011:  (numerical variable with categories [0,1,2])\n",
      "\n",
      "## Examples\n",
      "rs671 is 0. rs1426654 is 2. rs16891982 is 0. rs4988235 is 0. rs12913832 is 0. rs2814778 is 2. rs1042602 is 0. rs10498746 is 0. rs3827760 is 0. rs2192416 is 0. rs1390723 is 0. rs3814381 is 0. rs2476746 is 0. rs1878685 is 1. rs1344011 is 0.\n",
      "Answer: African Ancestry\n",
      "\n",
      "rs671 is 0. rs1426654 is 0. rs16891982 is 0. rs4988235 is 0. rs12913832 is 1. rs2814778 is 2. rs1042602 is 0. rs10498746 is 0. rs3827760 is 0. rs2192416 is 1. rs1390723 is 0. rs3814381 is 1. rs2476746 is 1. rs1878685 is 0. rs1344011 is 0.\n",
      "Answer: African Ancestry\n",
      "\n",
      "rs671 is 0. rs1426654 is 1. rs16891982 is 0. rs4988235 is 0. rs12913832 is 0. rs2814778 is 2. rs1042602 is 0. rs10498746 is 0. rs3827760 is 0. rs2192416 is 2. rs1390723 is 0. rs3814381 is 0. rs2476746 is 0. rs1878685 is 1. rs1344011 is 0.\n",
      "Answer: African Ancestry\n",
      "\n",
      "rs671 is 0. rs1426654 is 1. rs16891982 is 1. rs4988235 is 0. rs12913832 is 0. rs2814778 is 0. rs1042602 is 2. rs10498746 is 1. rs3827760 is 1. rs2192416 is 0. rs1390723 is 0. rs3814381 is 1. rs2476746 is 0. rs1878685 is 1. rs1344011 is 0.\n",
      "Answer: American Ancestry\n",
      "\n",
      "rs671 is 0. rs1426654 is 1. rs16891982 is 0. rs4988235 is 1. rs12913832 is 1. rs2814778 is 0. rs1042602 is 1. rs10498746 is 1. rs3827760 is 1. rs2192416 is 0. rs1390723 is 1. rs3814381 is 1. rs2476746 is 0. rs1878685 is 0. rs1344011 is 1.\n",
      "Answer: American Ancestry\n",
      "\n",
      "rs671 is 0. rs1426654 is 1. rs16891982 is 2. rs4988235 is 0. rs12913832 is 0. rs2814778 is 0. rs1042602 is 1. rs10498746 is 0. rs3827760 is 2. rs2192416 is 0. rs1390723 is 0. rs3814381 is 2. rs2476746 is 1. rs1878685 is 0. rs1344011 is 1.\n",
      "Answer: American Ancestry\n",
      "\n",
      "rs671 is 0. rs1426654 is 2. rs16891982 is 0. rs4988235 is 0. rs12913832 is 0. rs2814778 is 0. rs1042602 is 0. rs10498746 is 2. rs3827760 is 2. rs2192416 is 0. rs1390723 is 1. rs3814381 is 1. rs2476746 is 0. rs1878685 is 0. rs1344011 is 1.\n",
      "Answer: East Asian Ancestry\n",
      "\n",
      "rs671 is 1. rs1426654 is 2. rs16891982 is 0. rs4988235 is 0. rs12913832 is 0. rs2814778 is 0. rs1042602 is 0. rs10498746 is 0. rs3827760 is 2. rs2192416 is 1. rs1390723 is 0. rs3814381 is 1. rs2476746 is 0. rs1878685 is 0. rs1344011 is 0.\n",
      "Answer: East Asian Ancestry\n",
      "\n",
      "rs671 is 0. rs1426654 is 2. rs16891982 is 0. rs4988235 is 0. rs12913832 is 0. rs2814778 is 0. rs1042602 is 0. rs10498746 is 1. rs3827760 is 2. rs2192416 is 1. rs1390723 is 0. rs3814381 is 0. rs2476746 is 1. rs1878685 is 0. rs1344011 is 0.\n",
      "Answer: East Asian Ancestry\n",
      "\n",
      "rs671 is 0. rs1426654 is 0. rs16891982 is 2. rs4988235 is 1. rs12913832 is 2. rs2814778 is 0. rs1042602 is 1. rs10498746 is 0. rs3827760 is 0. rs2192416 is 0. rs1390723 is 1. rs3814381 is 2. rs2476746 is 0. rs1878685 is 0. rs1344011 is 0.\n",
      "Answer: European Ancestry\n",
      "\n",
      "rs671 is 0. rs1426654 is 0. rs16891982 is 2. rs4988235 is 1. rs12913832 is 0. rs2814778 is 0. rs1042602 is 1. rs10498746 is 1. rs3827760 is 0. rs2192416 is 0. rs1390723 is 0. rs3814381 is 1. rs2476746 is 1. rs1878685 is 0. rs1344011 is 0.\n",
      "Answer: European Ancestry\n",
      "\n",
      "rs671 is 0. rs1426654 is 0. rs16891982 is 2. rs4988235 is 1. rs12913832 is 1. rs2814778 is 0. rs1042602 is 2. rs10498746 is 0. rs3827760 is 0. rs2192416 is 0. rs1390723 is 1. rs3814381 is 0. rs2476746 is 0. rs1878685 is 0. rs1344011 is 0.\n",
      "Answer: European Ancestry\n",
      "\n",
      "rs671 is 0. rs1426654 is 0. rs16891982 is 0. rs4988235 is 0. rs12913832 is 0. rs2814778 is 0. rs1042602 is 1. rs10498746 is 1. rs3827760 is 0. rs2192416 is 1. rs1390723 is 1. rs3814381 is 0. rs2476746 is 0. rs1878685 is 0. rs1344011 is 0.\n",
      "Answer: South Asian Ancestry\n",
      "\n",
      "rs671 is 0. rs1426654 is 1. rs16891982 is 0. rs4988235 is 0. rs12913832 is 1. rs2814778 is 0. rs1042602 is 0. rs10498746 is 2. rs3827760 is 0. rs2192416 is 0. rs1390723 is 0. rs3814381 is 0. rs2476746 is 0. rs1878685 is 0. rs1344011 is 0.\n",
      "Answer: South Asian Ancestry\n",
      "\n",
      "rs671 is 0. rs1426654 is 1. rs16891982 is 0. rs4988235 is 0. rs12913832 is 0. rs2814778 is 0. rs1042602 is 0. rs10498746 is 1. rs3827760 is 0. rs2192416 is 1. rs1390723 is 0. rs3814381 is 0. rs2476746 is 1. rs1878685 is 0. rs1344011 is 0.\n",
      "Answer: South Asian Ancestry\n",
      "\n",
      "\n",
      "\n",
      "## Detailed Instructions\n",
      "Based on the above examples and your extensive knowledge, engineer a few features with some of the features listed above. Consider the following actions for feature engineering for each [Feature_name]:\n",
      "- Keep [Feature_name] as is\n",
      "- [Feature_name] is in [list of Categorical_values]\n",
      "- [Feature_name] (> or >= or < or <=) [Numerical_value]\n",
      "- [Feature1_name] + [Feature2_name] if you believe there is an additive effect\n",
      "- [Feature1_name] * [Feature2_name] if you believe there is a multiplicative effect \n",
      "- [Condition with Feature 1] AND [Condition with Feature 2] if you believe there's a tree-like interaction \n",
      "- Feel free to engineer other complex, creative interactions between features\n",
      "\n",
      "Here's a demonstration of how one might do feature engineering.\n",
      "\n",
      "rs12913832 (HERC2): The A allele denoted by rs1426654 is associated with lighter skin pigmentation, while the G allele is associated with darker skin pigmentation. The presence of the A allele is often used as a marker for European ancestry\n",
      "rs16891982 (SLC45A2): The G allele is associated with lighter skin, more common in Europeans, while the C allele is associated with darker skin tones and is more prevalent in other populations.\n",
      "Analysis of Synergistic Effect: When both the A allele at rs12913832 and the G allele at rs16891982 are present, it strongly suggests European ancestry, particularly from Northern Europe, where both light skin and blue eyes are common. However, this combination might be unexpectedly rare or absent in other European populations (e.g., Southern Europe), where other eye colors and skin tones are more prevalent.\n",
      "Output: interaction_rs12913832_rs16891982 = rs12913832 * rs16891982\n",
      "\n",
      "## Step-by-Step\n",
      "Let's first understand the problem and solve the problem step by step, with your own knowledge and without coding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "if 'ancestry' in _DATA:\n",
    "    _DATA_TYPE = 'ancestry'\n",
    "else:\n",
    "    _DATA_TYPE = 'hearing_loss'\n",
    "if \"gpt\" in _MODEL:\n",
    "    ask_file_name = f'./templates/ask_llm_{_PROMPT_VERSION}_{_DATA_TYPE}.txt'\n",
    "else: \n",
    "    ask_file_name = f'./templates/ask_llm_llama_{_PROMPT_VERSION}.txt'\n",
    "    \n",
    "meta_data_name = f\"../data/{_DATA}-metadata-{_METADATA_VERSION}.json\"\n",
    "templates, feature_desc = utils.get_prompt_for_asking(\n",
    "    _DATA, X_all, X_train, y_train, label_list, target_attr, ask_file_name, \n",
    "    meta_data_name, is_cat, num_query=_NUM_QUERY, num_conditions=_NUM_OF_CONDITIONS,\n",
    "    prompt_version =_PROMPT_VERSION\n",
    ")\n",
    "print(templates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6ee074a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:32<00:00, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, let's proceed step by step to analyze and engineer features from the provided SNPs (single nucleotide polymorphisms) to predict the genomic ancestry of individuals. Here's a structured approach:\n",
      "\n",
      "### Step 1: Understand the Role of Individual SNPs\n",
      "1. **rs671 (ALDH2)**: Known to be a strong marker in East Asian populations.\n",
      "2. **rs1426654 (SLC24A5)**: Associated with lighter skin pigmentation, common in European populations.\n",
      "3. **rs16891982 (SLC45A2)**: The G allele is associated with lighter skin pigmentation, typically found in Europeans.\n",
      "4. **rs4988235 (LCT)**: The T allele indicates lactase persistence, common in European populations.\n",
      "5. **rs12913832 (HERC2/OCA2)**: The A allele is associated with lighter eye color, particularly blue, common in Northern Europeans.\n",
      "6. **rs2814778 (DARC)**: The T allele indicates resistance to malaria, common in African populations.\n",
      "7. **rs1042602 (TYR)**: The A allele is associated with lighter skin, common in various populations.\n",
      "8. **rs10498746 (EDAR)**: Associated with hair and teeth morphology, common in East Asians.\n",
      "9. **rs3827760 (EDAR)**: Also linked to hair morphology in East Asians.\n",
      "10. **rs2192416 (ALOX5)**: Varies across populations.\n",
      "11. **rs1390723 (Grm6)**: Specific allele distributions may vary.\n",
      "12. **rs3814381 (ADH1B)**: Variations are population-specific.\n",
      "13. **rs2476746 (RPTN)**: Variations may suggest regional differences.\n",
      "14. **rs1878685 (GIPC1)**: Relevant allele distributions suggest variations.\n",
      "15. **rs1344011 (FUT2)**: Allele frequency differs across populations.\n",
      "\n",
      "### Step 2: Feature Engineering\n",
      "We can proceed to create novel features and potential interactions based on the given SNP contributions to identifying genomic ancestry groups.\n",
      "\n",
      "#### 1. Individual Features:\n",
      "- **rs671**: Retain as is.\n",
      "- **rs1426654**: Retain as is.\n",
      "- **rs16891982**: Retain as is.\n",
      "- **rs4988235**: Retain as is.\n",
      "- **rs12913832**: Retain as is.\n",
      "- **rs2814778**: Retain as is.\n",
      "- **rs10498746**: Retain as is.\n",
      "- **rs3827760**: Retain as is.\n",
      "\n",
      "#### 2. Interaction Features:\n",
      "- **Europe-specific Interactions**:\n",
      "  - **rs1426654 * rs16891982**: Indicates the joint effect of skin pigmentation genes.\n",
      "  \n",
      "- **East Asia-specific Interactions**:\n",
      "  - **rs671 * rs10498746**: Joint effect on alcohol metabolism and morphological traits.\n",
      "  - **rs10498746 + rs3827760**: Combined effect of two EDAR-related morphological SNPs.\n",
      "  \n",
      "- **African-specific Interactions**:\n",
      "  - **rs2814778 * rs12913832**: Combining a marker for malaria resistance with an eye color marker.\n",
      "  \n",
      "- **American-specific Interactions**:\n",
      "  - **rs1042602 * rs1344011**: Combined effect on skin pigmentation and secretor status.\n",
      "\n",
      "#### 3. Threshold Features:\n",
      "- Threshold-based new features based on typical allele values:\n",
      "  - **rs1426654 ≥ 1**: Indicative of lighter skin pigmentation, common in Europeans.\n",
      "  - **rs12913832 ≥ 1**: Suggests lighter eye color, particularly blue, prevalent in Europeans.\n",
      "\n",
      "### Step 3: Decision Tree-Like Conditions\n",
      "Based on tree-like feature functions, if both conditions in SNP pairs are met, it strongly suggests certain ancestry.\n",
      "\n",
      "- **(rs671 ≥ 1) AND (rs10498746 ≥ 1)**: Highly indicative of East Asian ancestry.\n",
      "- **(rs1426654 ≥ 1) AND (rs16891982 ≥ 1)**: Strong signal for European ancestry.\n",
      "- **(rs2814778 ≥ 1) AND (rs12913832 ≤ 1)**: Indicates African ancestry.\n",
      "\n",
      "### Step 4: Summary of Engineered Features\n",
      "1. **Direct Features**: All original SNP values.\n",
      "2. **Interaction Features**: \n",
      "   - **interaction_rs1426654_rs16891982**: rs1426654 * rs16891982.\n",
      "   - **interaction_rs671_rs10498746**: rs671 * rs10498746.\n",
      "   - **sum_rs10498746_rs3827760**: rs10498746 + rs3827760.\n",
      "   - **interaction_rs2814778_rs12913832**: rs2814778 * rs12913832.\n",
      "   - **interaction_rs1042602_rs1344011**: rs1042602 * rs1344011.\n",
      "3. **Threshold-Based Features**:\n",
      "   - **rs1426654_ge_1**: rs1426654 ≥ 1.\n",
      "   - **rs12913832_ge_1**: rs12913832 ≥ 1.\n",
      "4. **Tree-like Interaction Features**:\n",
      "   - **is_east_asian**: (rs671 ≥ 1) AND (rs10498746 ≥ 1).\n",
      "   - **is_european**: (rs1426654 ≥ 1) AND (rs16891982 ≥ 1).\n",
      "   - **is_african**: (rs2814778 ≥ 1) AND (rs12913832 ≤ 1).\n",
      "\n",
      "### Conclusion\n",
      "The engineered features from individual SNPs, their interactions, threshold-based indicators, and tree-like combinations collectively build a robust feature set to classify genomic ancestry accurately. \n",
      "\n",
      "This approach leverages the biological significance of each SNP and their known interaction effects to capture complex patterns underlying genomic ancestry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_DIVIDER = \"\\n\\n---DIVIDER---\\n\\n\"\n",
    "_VERSION = \"\\n\\n---VERSION---\\n\\n\"\n",
    "\n",
    "rule_file_name = f'./rules/{_DATA}/{_SHOT}_shot/rule-s{_SHOT}-c{_NUM_OF_CONDITIONS}{_PROMPT_VERSION}-{_MODEL}-q{_NUM_QUERY}-{_SEED}{_NOTE}.out'\n",
    "if os.path.isfile(rule_file_name) == False:\n",
    "    results = utils.query_gpt(templates, max_tokens=2000, temperature=1, model = _MODEL)\n",
    "    if _RECORD_LOGS:\n",
    "        with open(rule_file_name, 'w') as f:\n",
    "            total_rules = _DIVIDER.join(results)\n",
    "            f.write(total_rules)\n",
    "else:\n",
    "    with open(rule_file_name, 'r') as f:\n",
    "        total_rules_str = f.read().strip()\n",
    "        results = total_rules_str.split(_DIVIDER)\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "86263ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.96s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.98s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.20s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.00s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.20s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['- rs671\\n- rs1426654\\n- rs16891982\\n- rs4988235\\n- rs12913832\\n- rs2814778\\n- rs10498746\\n- rs3827760\\n- interaction_rs1426654_rs16891982: rs1426654 * rs16891982\\n- interaction_rs671_rs10498746: rs671 * rs10498746\\n- sum_rs10498746_rs3827760: rs10498746 + rs3827760\\n- interaction_rs2814778_rs12913832: rs2814778 * rs12913832\\n- interaction_rs1042602_rs1344011: rs1042602 * rs1344011\\n- rs1426654_ge_1: rs1426654 ≥ 1\\n- rs12913832_ge_1: rs12913832 ≥ 1\\n- is_east_asian: (rs671 ≥ 1) AND (rs10498746 ≥ 1)\\n- is_european: (rs1426654 ≥ 1) AND (rs16891982 ≥ 1)\\n- is_african: (rs2814778 ≥ 1) AND (rs12913832 ≤ 1)', '- rs12913832 * rs16891982\\n- rs671 + rs10498746\\n- rs2814778 + rs1878685\\n- rs1426654 * rs16891982 * rs4988235\\n- rs1426654 + rs16891982\\n- rs1390723 + rs3814381\\n- rs12913832 AND rs16891982\\n- rs2814778 == 2 AND rs1878685 == 1\\n- rs671 == 1 AND rs10498746 >= 1', '- rs1426654\\n- rs16891982\\n- rs4988235\\n- European_marker = (rs1426654 >= 1) + (rs16891982 >= 1) + (rs4988235 >= 1)\\n- African_marker = (rs2814778 >= 1)\\n- interaction_rs12913832_rs16891982 = rs12913832 * rs16891982', '- feat_eu_1: rs12913832 * rs16891982\\n- feat_eu_2: rs16891982 + rs4988235\\n- feat_af_1: rs2814778 == 2\\n- feat_af_2: rs2814778 + rs1426654\\n- feat_ea_1: rs671 * rs1426654\\n- feat_am_1: rs4988235 + rs16791982\\n- feat_am_2: rs10498746 + rs3827760\\n- feat_sa_1: rs1426654 * rs1042602\\n- feat_sa_2: rs12913832 + rs10498746', '- **interaction_skin_pigmentation:** `rs1426654 * rs16891982`\\n- **east_asian_score:** `rs671 + rs3827760`\\n- **african_score:** `rs2814778`\\n- **lactase_persistence:** `rs4988235`\\n- **eye_hair_european_score:** `rs12913832 * rs1426654`\\n- **american_score:** `rs1426654 * rs16891982`', '- rs1426654\\n- rs16891982\\n- interaction_rs1426654_rs16891982 = rs1426654 * rs16891982\\n- interaction_rs12913832_rs1426654 = rs12913832 * rs1426654\\n- interaction_rs671_rs10498746 = (rs671 == 1) and (rs10498746 == 1)\\n- sum_european_snps = rs1426654 + rs16891982 + rs12913832 + rs4988235\\n- sum_east_asian_snps = rs671 + rs10498746 + rs3827760', '1. interaction_rs12913832_rs16891982\\n2. interaction_rs671_rs3827760\\n3. combined_rs1426654_rs2814778\\n4. lactase_persistence_rs4988235\\n5. multiplicative_marker_rs1426654_rs1042602', '- `interaction_rs12913832_rs1426654 = rs12913832 * rs1426654`\\n- `interaction_rs4988235_rs1426654 = rs4988235 * rs1426654`\\n- `east_asian_indicator = (rs671 == 2) AND (rs3827760 == 2)`\\n- `african_indicator = (rs2814778 == 2)`\\n- `pigmentation_sum = rs1426654 + rs16891982 + rs12913832 + rs1042602 + rs1390723`\\n- `american_combination = rs16891982 * rs10498746`', '- `interaction_rs12913832_rs16891982` = `rs12913832` * `rs16891982`\\n- `interaction_rs671_rs3827760` = `rs671` * `rs3827760`\\n- `interaction_rs1426654_rs16891982` = `rs1426654` * `rs16891982`\\n- `interaction_rs2814778_rs1426654` = `rs2814778` * `rs1426654`\\n- `interaction_rs4988235_rs12913832` = `rs4988235` * `rs12913832`\\n- `interaction_rs12913832_rs2814778` = `rs12913832` * `rs2814778`\\n- `sum_rs1426654_rs16891982_rs12913832` = `rs1426654` + `rs16891982` + `rs12913832`\\n- `sum_rs671_rs3827760` = `rs671` + `rs3827760`\\n- `sum_rs2814778_rs1426654` = `rs2814778` + `rs1426654`', 'engineered_feature_1 = rs12913832 * rs16891982\\nengineered_feature_2 = rs4988235 (>= 1)\\nengineered_feature_3 = (rs2814778 >= 1)\\nengineered_feature_4 = (rs1426654 == 0) * (rs12913832 != 2)\\nengineered_feature_5 = rs671 * rs3827760\\nengineered_feature_6 = rs1426654 * rs10498746\\nengineered_feature_7 = (rs1426654 == 1) * rs2192416\\nengineered_feature_8 = rs12913832 * rs4988235\\nengineered_feature_9 = rs3814381 * rs1878685', '1. `interaction_hair_skin = rs12913832 * rs1426654`\\n2. `interaction_skin_light = rs1426654 * rs16891982`\\n3. `interaction_lactase_skin = rs1426654 * rs4988235`\\n4. `interaction_alcohol_pigment = rs671 * rs1426654`\\n5. `interaction_european_triplet = rs12913832 * rs1426654 * rs16891982`\\n6. `interaction_tree_african_nonAmerican = (rs2814778 == 2) * (rs1426654 <= 1)`', '- interaction_rs1426654_rs16891982 = rs1426654 * rs16891982\\n- interaction_rs12913832_rs16891982 = rs12913832 * rs16891982\\n- interaction_rs4988235_rs12913832 = rs4988235 * rs12913832\\n- interaction_rs10498746_rs3827760 = rs10498746 * rs3827760', '- **rs1426654_light_skin** =  (rs1426654 >= 1)\\n- **rs2814778_african_marker** =  (rs2814778 == 2)\\n- **interaction_rs1426654_rs16891982** = rs1426654 * rs16891982\\n- **interaction_rs671_rs3827760** = rs671 * rs3827760\\n- **high_rs1426654_and_rs16891982** = (rs1426654 >= 1) & (rs16891982 >= 1)\\n- **rs2814778_and_rs1042602_dark_skin** = (rs2814778 == 2) & (rs1042602 >= 1)\\n- **pigmentation_signature** = rs1426654 + rs16891982 + rs1042602 + rs10498746\\n- **european_signature** = rs1426654 + rs16891982 + rs4988235 + rs12913832', '- interaction_rs1426654_rs16891982\\n- interaction_rs12913832_rs4988235\\n- interaction_rs3827760_rs671\\n- Sum_SNP_1 = rs1426654 + rs12913832 + rs4988235\\n- rs671 > 0 - East Asian Indicator\\n- rs4988235 > 0 - Lactose Tolerant Indicator (European)\\n- rs2814778 == 2 - African Indicator (Duffy-null system)\\n- rs3827760 > 0 - East Asian Feature (Hair thickness)', '- rs1426654_0_or_1\\n- interaction_rs12913832_rs16891982\\n- interaction_rs671_rs3827760\\n- rs1426654_is_0_AND_rs2814778_is_2\\n- rs16891982_is_2_AND_rs12913832_is_2\\n- sum_rs4988235_rs2192416\\n- sum_rs10498746_rs1426654_rs16891982', '- rs671 (ALDH2)\\n- rs1426654 (SLC24A5)\\n- rs16891982 (SLC45A2)\\n- rs4988235 (LCT)\\n- rs12913832 (HERC2/OCA2)\\n- rs2814778 (DARC)\\n- rs1042602 (TYR)\\n- rs10498746 (ADH1B)\\n- rs3827760 (EDAR)\\n- interaction_european = rs1426654 * rs16891982\\n- interaction_east_asian = rs671 * rs3827760\\n- interaction_pigmentation = rs1426654 + rs16891982 + rs1042602 + rs10498746 + rs12913832\\n- ancestry_score = interaction_european + interaction_east_asian - rs2814778', '1. east_asian = rs671 + rs3827760\\n2. african = rs2814778\\n3. interaction_rs1426654_rs16891982\\n4. interaction_rs12913832_rs4988235\\n5. complex_interaction = rs3814381 + rs2192416 + rs1390723', '- binary_rs1426654 = 1 if rs1426654 == 2 else 0\\n- binary_rs2814778 = 1 if rs2814778 == 2 else 0\\n- interaction_rs12913832_rs1426654 = rs12913832 * rs1426654\\n- interaction_rs16891982_rs12913832 = rs16891982 * rs12913832\\n- rs1426654_ge1 = 1 if rs1426654 >= 1 else 0\\n- rs4988235_rs1426654 = rs4988235 * rs1426654\\n- tree_interaction_1 = 1 if (rs1426654 == 2 and rs16891982 == 2) else 0\\n- tree_interaction_2 = 1 if (rs1042602 == 2 and rs1426654 == 1) else 0', '- rs1426654 >= 2\\n- rs16891982 >= 1\\n- rs12913832 + rs2814778\\n- rs1426654 * rs16891982 * rs12913832\\n- (rs1426654 >= 2) AND (rs16891982 >= 2)\\n- (rs2814778 = 2)\\n- (rs1426654 * rs2827740)\\n- rs12913832 + rs1042602', '- rs671\\n- rs1426654\\n- rs16891982\\n- rs4988235\\n- rs12913832\\n- rs2814778\\n- rs1042602\\n- rs10498746\\n- rs3827760\\n- rs2192416\\n- rs1390723\\n- rs3814381\\n- rs2476746\\n- rs1878685\\n- rs1344011\\n\\n- rs2814778 >= 2 → African indicator\\n- rs1426654 <= 1 AND rs12913832 <= 1 → South Asian indicator\\n\\n- interaction_rs1426654_rs12913832 = rs1426654 * rs12913832\\n- interaction_rs1426654_rs16891982 = rs1426654 * rs16891982\\n- triple_interaction = rs1426654 + rs16891982 + rs12913832\\n- rs16891982 * rs4988235 → American-related interaction\\n- rs671 == 1 OR (rs3814381 >= 1 AND rs1426654 == 2) → East Asian indicator']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_rules = []\n",
    "\n",
    "# Iterate through each result in the results list\n",
    "for result in results:\n",
    "    # Use utils.query_gpt to transform each result\n",
    "    transformed_result = utils.query_gpt(\n",
    "        [f\"Extract the list of engineered features and list them one after another in a new line: {result}\\n\\nIf some features are clumped up together, list them separately. Also, make sure to include the equation/instruction for each feature. \\n\\nList:\"], \n",
    "        max_tokens=2000, \n",
    "        temperature=0, \n",
    "        model=_FUNCTION_MODEL\n",
    "    )\n",
    "    # Append the transformed result to the results_transformed list\n",
    "    parsed_rules.append(transformed_result[0])\n",
    "\n",
    "# The parsed_rules list now contains all the transformed results\n",
    "print(parsed_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e00f3eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- rs671\n",
      "- rs1426654\n",
      "- rs16891982\n",
      "- rs4988235\n",
      "- rs12913832\n",
      "- rs2814778\n",
      "- rs10498746\n",
      "- rs3827760\n",
      "- interaction_rs1426654_rs16891982: rs1426654 * rs16891982\n",
      "- interaction_rs671_rs10498746: rs671 * rs10498746\n",
      "- sum_rs10498746_rs3827760: rs10498746 + rs3827760\n",
      "- interaction_rs2814778_rs12913832: rs2814778 * rs12913832\n",
      "- interaction_rs1042602_rs1344011: rs1042602 * rs1344011\n",
      "- rs1426654_ge_1: rs1426654 ≥ 1\n",
      "- rs12913832_ge_1: rs12913832 ≥ 1\n",
      "- is_east_asian: (rs671 ≥ 1) AND (rs10498746 ≥ 1)\n",
      "- is_european: (rs1426654 ≥ 1) AND (rs16891982 ≥ 1)\n",
      "- is_african: (rs2814778 ≥ 1) AND (rs12913832 ≤ 1)\n"
     ]
    }
   ],
   "source": [
    "print(parsed_rules[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a316dc4",
   "metadata": {},
   "source": [
    "## Parse rules to the program code and convert data into the binary vector\n",
    "\n",
    "We utilize the rules generated in the previous stage to transform each sample into a binary vector. These vectors are created for each answer class, indicating whether the sample satisfies the rules associated with that class. However, since the rules generated by the LLM are based on natural language, parsing the text into program code is required for automatic data transformation.  \n",
    "\n",
    "To address the challenges of parsing noisy text, instead of building complex program code, we leverage the LLM itself. We include the function name, input and output descriptions, and inferred rules in the prompt, then input it into the LLM. The generated code is executed using Python’s exec() function along with the provided function name to perform data conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "69c7e756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.28s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.91s/it]]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.12s/it]]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.66s/it]]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.23s/it]]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.87s/it]]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.35s/it]]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.86s/it]]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.23s/it]]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.51s/it]]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.15s/it]t]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.61s/it]t]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.47s/it]t]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.78s/it]t]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.44s/it]t]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.47s/it]t]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.01s/it]t]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.70s/it]t]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.44s/it]t]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.80s/it]t]\n",
      "100%|██████████| 20/20 [01:09<00:00,  3.45s/it]\n"
     ]
    }
   ],
   "source": [
    "reload(utils)\n",
    "_DIVIDER = \"\\n\\n---DIVIDER---\\n\\n\"\n",
    "_VERSION = \"\\n\\n---VERSION---\\n\\n\"\n",
    "\n",
    "saved_file_name = f'./rules/{_DATA}/{_SHOT}_shot/function-s{_SHOT}-c{_NUM_OF_CONDITIONS}{_PROMPT_VERSION}-{_MODEL}-{_FUNCTION_MODEL}-q{_NUM_QUERY}-{_SEED}{_NOTE}.out'    \n",
    "if os.path.isfile(saved_file_name) == False:\n",
    "    function_file_name = './templates/ask_for_function_v2.txt'\n",
    "    fct_strs_all = []\n",
    "    for parsed_rule in tqdm(parsed_rules):\n",
    "        fct_template = utils.get_prompt_for_generating_function_simple(\n",
    "            parsed_rule, feature_desc, function_file_name\n",
    "        )\n",
    "        fct_results = utils.query_gpt(fct_template, max_tokens=2500, temperature=0, model = _FUNCTION_MODEL)\n",
    "        fct_strs = [fct_txt.split('<start>')[1].split('<end>')[0].strip() for fct_txt in fct_results]\n",
    "        fct_strs_all.append(fct_strs[0])\n",
    "    if _RECORD_LOGS:\n",
    "        with open(saved_file_name, 'w') as f:\n",
    "            total_str = _VERSION.join([x for x in fct_strs_all])\n",
    "            f.write(total_str)\n",
    "else:\n",
    "    with open(saved_file_name, 'r') as f:\n",
    "        total_str = f.read().strip()\n",
    "        fct_strs_all = [x for x in total_str.split(_VERSION)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6104e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def extracting_engineered_features(df_input):\n",
      "    df_output = pd.DataFrame()\n",
      "    df_output['rs671'] = df_input['rs671']\n",
      "    df_output['rs1426654'] = df_input['rs1426654']\n",
      "    df_output['rs16891982'] = df_input['rs16891982']\n",
      "    df_output['rs4988235'] = df_input['rs4988235']\n",
      "    df_output['rs12913832'] = df_input['rs12913832']\n",
      "    df_output['rs2814778'] = df_input['rs2814778']\n",
      "    df_output['rs10498746'] = df_input['rs10498746']\n",
      "    df_output['rs3827760'] = df_input['rs3827760']\n",
      "    df_output['interaction_rs1426654_rs16891982'] = df_input['rs1426654'] * df_input['rs16891982']\n",
      "    df_output['interaction_rs671_rs10498746'] = df_input['rs671'] * df_input['rs10498746']\n",
      "    df_output['sum_rs10498746_rs3827760'] = df_input['rs10498746'] + df_input['rs3827760']\n",
      "    df_output['interaction_rs2814778_rs12913832'] = df_input['rs2814778'] * df_input['rs12913832']\n",
      "    df_output['interaction_rs1042602_rs1344011'] = df_input['rs1042602'] * df_input['rs1344011']\n",
      "    df_output['rs1426654_ge_1'] = df_input['rs1426654'].apply(lambda x: 1 if x >= 1 else 0)\n",
      "    df_output['rs12913832_ge_1'] = df_input['rs12913832'].apply(lambda x: 1 if x >= 1 else 0)\n",
      "    df_output['is_east_asian'] = df_input.apply(lambda row: 1 if row['rs671'] >= 1 and row['rs10498746'] >= 1 else 0, axis=1)\n",
      "    df_output['is_european'] = df_input.apply(lambda row: 1 if row['rs1426654'] >= 1 and row['rs16891982'] >= 1 else 0, axis=1)\n",
      "    df_output['is_african'] = df_input.apply(lambda row: 1 if row['rs2814778'] >= 1 and row['rs12913832'] <= 1 else 0, axis=1)\n",
      "    \n",
      "    return df_output\n"
     ]
    }
   ],
   "source": [
    "# Examine function output\n",
    "print(fct_strs_all[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08cd8ce",
   "metadata": {},
   "source": [
    "#### Self-Critiqueing Function Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3cc801bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "critique_fct_strs_all = utils.self_critique_functions(parsed_rules, feature_desc, fct_strs_all, X_train, _NUM_OF_CONDITIONS, _NUM_OF_CONDITIONS_FOR_INTERACTIONS, _REWRITING_FUNCTION_MODEL, condition_tolerance=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "20a2690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _RECORD_LOGS:\n",
    "    with open(saved_file_name, 'w') as f:\n",
    "        total_str = _VERSION.join([x for x in critique_fct_strs_all])\n",
    "        f.write(total_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "02ae86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get function names and strings\n",
    "fct_names = []\n",
    "fct_strs_final = []\n",
    "for fct_str in critique_fct_strs_all:\n",
    "    if 'def' not in fct_str:\n",
    "        continue\n",
    "    fct_names.append(fct_str.split('def')[1].split('(')[0].strip())\n",
    "    fct_strs_final.append(fct_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c1e34",
   "metadata": {},
   "source": [
    "### Generating Transformed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7f9d8141",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = X_test.notna().all(axis=1)\n",
    "\n",
    "# Dropping weird NAs\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "07cb55be",
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_list, X_train_all_dict, X_test_all_dict = utils.convert_to_binary_vectors_simple(fct_strs_final, \n",
    "                                                                                     fct_names, \n",
    "                                                                                     X_train, \n",
    "                                                                                     X_test, \n",
    "                                                                                     num_of_features=_NUM_OF_CONDITIONS,\n",
    "                                                                                     include_original_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d781a3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of functions should be == # of transformed datasets. If lower than expected, some of the functions are faulty and were dropped\n",
    "len(executable_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea38b2b",
   "metadata": {},
   "source": [
    "## Train the linear model to predict the likelihood of each class from the binary vector\n",
    "When given the rules for each class and a sample, a simple method to measure the class likelihood of the sample is to count how many rules of each class it satisfies (i.e., the sum of the binary vector per class). However, not all rules carry the same importance, necessitating learning their significance from training samples.    \n",
    "  \n",
    "We aimed to train this importance using a basic linear model without bias, applied to each class's binary vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "19dbdcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.969554334497049,\n",
       " 'Accuracy': 0.8706070287539937,\n",
       " 'F1-Score': 0.8506037121536921,\n",
       " 'Class African Ancestry Precision': 0.9589442815249267,\n",
       " 'Class African Ancestry Recall': 0.9879154078549849,\n",
       " 'Class African Ancestry F1-Score': 0.9732142857142857,\n",
       " 'Class American Ancestry Precision': 0.8524590163934426,\n",
       " 'Class American Ancestry Recall': 0.30057803468208094,\n",
       " 'Class American Ancestry F1-Score': 0.4444444444444445,\n",
       " 'Class East Asian Ancestry Precision': 0.8723404255319149,\n",
       " 'Class East Asian Ancestry Recall': 0.9761904761904762,\n",
       " 'Class East Asian Ancestry F1-Score': 0.9213483146067415,\n",
       " 'Class European Ancestry Precision': 0.775,\n",
       " 'Class European Ancestry Recall': 0.9841269841269841,\n",
       " 'Class European Ancestry F1-Score': 0.8671328671328671,\n",
       " 'Class South Asian Ancestry Precision': 0.875,\n",
       " 'Class South Asian Ancestry Recall': 0.889344262295082,\n",
       " 'Class South Asian Ancestry F1-Score': 0.8821138211382114}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "multiclass = True if len(label_list) > 2 else False\n",
    "y_train_num = np.array([label_list.index(k) for k in y_train])\n",
    "y_test_num = np.array([label_list.index(k) for k in y_test])\n",
    "single_model = LogisticRegression(random_state=_SEED)\n",
    "\n",
    "# Fit the model\n",
    "single_model.fit(X_train, y_train_num)\n",
    "lr_pred_probs_train = single_model.predict_proba(X_train)\n",
    "lr_metrics_train = utils.evaluate(lr_pred_probs_train, y_train_num, multiclass=multiclass, class_level_analysis=True, label_list=label_list)\n",
    "lr_pred_probs_test = single_model.predict_proba(X_test)\n",
    "lr_metrics_test = utils.evaluate(lr_pred_probs_test, y_test_num, multiclass=multiclass, class_level_analysis=True, label_list=label_list)\n",
    "lr_metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c170073d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of features:  33\n",
      "0.9697826369775411\n",
      "num of features:  24\n",
      "0.9635840445335706\n",
      "num of features:  21\n",
      "0.9680005236557422\n",
      "num of features:  24\n",
      "0.9702244901307093\n",
      "num of features:  21\n",
      "0.9701163242912226\n",
      "num of features:  22\n",
      "0.9655161057134105\n",
      "num of features:  20\n",
      "0.9695989813395048\n",
      "num of features:  21\n",
      "0.9647511094239517\n",
      "num of features:  24\n",
      "0.9717911408647584\n",
      "num of features:  24\n",
      "0.9641433621424074\n",
      "num of features:  21\n",
      "0.97032004497116\n",
      "num of features:  34\n",
      "0.9686770495844191\n",
      "num of features:  23\n",
      "0.9595605871568985\n",
      "num of features:  23\n",
      "0.9694344855483854\n",
      "num of features:  22\n",
      "0.9660964290859682\n",
      "num of features:  28\n",
      "0.9705685493466685\n",
      "num of features:  20\n",
      "0.9707145224572409\n",
      "num of features:  23\n",
      "0.9705227367411494\n",
      "num of features:  23\n",
      "0.9743781348988744\n",
      "num of features:  37\n",
      "0.9712535050934834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.9730741192810519,\n",
       " 'Accuracy': 0.8753993610223643,\n",
       " 'F1-Score': 0.8562916444502596,\n",
       " 'Class African Ancestry Precision': 0.9646017699115044,\n",
       " 'Class African Ancestry Recall': 0.9879154078549849,\n",
       " 'Class African Ancestry F1-Score': 0.9761194029850746,\n",
       " 'Class American Ancestry Precision': 0.8059701492537313,\n",
       " 'Class American Ancestry Recall': 0.31213872832369943,\n",
       " 'Class American Ancestry F1-Score': 0.45,\n",
       " 'Class East Asian Ancestry Precision': 0.8848920863309353,\n",
       " 'Class East Asian Ancestry Recall': 0.9761904761904762,\n",
       " 'Class East Asian Ancestry F1-Score': 0.9283018867924528,\n",
       " 'Class European Ancestry Precision': 0.8071895424836601,\n",
       " 'Class European Ancestry Recall': 0.9801587301587301,\n",
       " 'Class European Ancestry F1-Score': 0.8853046594982079,\n",
       " 'Class South Asian Ancestry Precision': 0.8473282442748091,\n",
       " 'Class South Asian Ancestry Recall': 0.9098360655737705,\n",
       " 'Class South Asian Ancestry F1-Score': 0.8774703557312252}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "models = []\n",
    "\n",
    "# Train an MLP model on each version of the training data\n",
    "for X_train_now, X_test_now in zip(X_train_all_dict.values(), X_test_all_dict.values()):\n",
    "    model = LogisticRegression(random_state=_SEED)\n",
    "    model.fit(X_train_now, y_train_num)\n",
    "    models.append(model)\n",
    "    lr_pred_probs_train = model.predict_proba(X_train_now)\n",
    "    lr_metrics_train = utils.evaluate(lr_pred_probs_train, y_train_num, multiclass=multiclass, class_level_analysis=True, label_list=label_list)\n",
    "    lr_pred_probs_test = model.predict_proba(X_test_now)\n",
    "    lr_metrics_test = utils.evaluate(lr_pred_probs_test, y_test_num, multiclass=multiclass, class_level_analysis=True, label_list=label_list)\n",
    "    print(\"num of features: \", X_train_now.shape[1])\n",
    "    print(lr_metrics_test['AUC'])\n",
    "\n",
    "# Initialize arrays to store ensemble predictions\n",
    "ensemble_pred_probs_train = np.zeros((X_train_all_dict[0].shape[0], len(label_list)))\n",
    "ensemble_pred_probs_test = np.zeros((X_test_all_dict[0].shape[0], len(label_list)))\n",
    "\n",
    "# Predict probabilities for training and test sets using each model and combine them\n",
    "for i, (X_train_now, X_test_now) in enumerate(zip(X_train_all_dict.values(), X_test_all_dict.values())):\n",
    "    ensemble_pred_probs_train += models[i].predict_proba(X_train_now)\n",
    "    ensemble_pred_probs_test += models[i].predict_proba(X_test_now)\n",
    "    \n",
    "\n",
    "# Average the probabilities\n",
    "ensemble_pred_probs_train /= len(X_train_all_dict)\n",
    "ensemble_pred_probs_test /= len(X_test_all_dict)\n",
    "\n",
    "# Evaluate the ensemble predictions\n",
    "ensemble_metrics_train = utils.evaluate(\n",
    "    ensemble_pred_probs_train, \n",
    "    y_train_num, \n",
    "    multiclass=multiclass, \n",
    "    class_level_analysis=True, \n",
    "    label_list=label_list\n",
    ")\n",
    "\n",
    "ensemble_metrics_test = utils.evaluate(\n",
    "    ensemble_pred_probs_test, \n",
    "    y_test_num, \n",
    "    multiclass=multiclass, \n",
    "    class_level_analysis=True, \n",
    "    label_list=label_list\n",
    ")\n",
    "\n",
    "# Output the test metrics\n",
    "ensemble_metrics_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87544d02",
   "metadata": {},
   "source": [
    "### Random Forest Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e20488d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.9578963880654356,\n",
       " 'Accuracy': 0.8538338658146964,\n",
       " 'F1-Score': 0.8405902174391501,\n",
       " 'Class African Ancestry Precision': 0.9480122324159022,\n",
       " 'Class African Ancestry Recall': 0.9365558912386707,\n",
       " 'Class African Ancestry F1-Score': 0.9422492401215805,\n",
       " 'Class American Ancestry Precision': 0.7764705882352941,\n",
       " 'Class American Ancestry Recall': 0.3815028901734104,\n",
       " 'Class American Ancestry F1-Score': 0.5116279069767441,\n",
       " 'Class East Asian Ancestry Precision': 0.8531468531468531,\n",
       " 'Class East Asian Ancestry Recall': 0.9682539682539683,\n",
       " 'Class East Asian Ancestry F1-Score': 0.9070631970260223,\n",
       " 'Class European Ancestry Precision': 0.78125,\n",
       " 'Class European Ancestry Recall': 0.9920634920634921,\n",
       " 'Class European Ancestry F1-Score': 0.8741258741258742,\n",
       " 'Class South Asian Ancestry Precision': 0.8504273504273504,\n",
       " 'Class South Asian Ancestry Recall': 0.8155737704918032,\n",
       " 'Class South Asian Ancestry F1-Score': 0.8326359832635982}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "multiclass = True if len(label_list) > 2 else False\n",
    "y_train_num = np.array([label_list.index(k) for k in y_train])\n",
    "y_test_num = np.array([label_list.index(k) for k in y_test])\n",
    "single_model = RandomForestClassifier(random_state=_SEED)\n",
    "\n",
    "# Fit the model\n",
    "single_model.fit(X_train, y_train_num)\n",
    "lr_pred_probs_train = single_model.predict_proba(X_train)\n",
    "lr_metrics_train = utils.evaluate(lr_pred_probs_train, y_train_num, multiclass=multiclass, class_level_analysis=True, label_list=label_list)\n",
    "lr_pred_probs_test = single_model.predict_proba(X_test)\n",
    "lr_metrics_test = utils.evaluate(lr_pred_probs_test, y_test_num, multiclass=multiclass, class_level_analysis=True, label_list=label_list)\n",
    "lr_metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "83a7b8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plots/tree.pdf'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "estimator = single_model.estimators_[4]\n",
    "\n",
    "# Export the tree to Graphviz format\n",
    "dot_data = export_graphviz(estimator, out_file=None,\n",
    "                           class_names=label_list,\n",
    "                           filled=True, rounded=True,\n",
    "                           special_characters=True)\n",
    "\n",
    "# Visualize the tree using graphviz\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(\"plots/tree\")  # Save the tree as a file\n",
    "graph.view()  # View the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5da83ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of features:  33\n",
      "0.9604055638360265\n",
      "num of features:  24\n",
      "0.963850644330997\n",
      "num of features:  21\n",
      "0.9651194211347158\n",
      "num of features:  24\n",
      "0.9608693997409599\n",
      "num of features:  21\n",
      "0.9658239533808324\n",
      "num of features:  22\n",
      "0.9592185415826915\n",
      "num of features:  20\n",
      "0.9648763181844668\n",
      "num of features:  21\n",
      "0.965003526008376\n",
      "num of features:  24\n",
      "0.9668977361621801\n",
      "num of features:  24\n",
      "0.9650423256509164\n",
      "num of features:  21\n",
      "0.9587053244582086\n",
      "num of features:  34\n",
      "0.9640501067793199\n",
      "num of features:  23\n",
      "0.9550881419397588\n",
      "num of features:  23\n",
      "0.9624929262985589\n",
      "num of features:  22\n",
      "0.9608536632277399\n",
      "num of features:  28\n",
      "0.9629612690313311\n",
      "num of features:  20\n",
      "0.9625805686343346\n",
      "num of features:  23\n",
      "0.9581236817206495\n",
      "num of features:  23\n",
      "0.9718080775111352\n",
      "num of features:  37\n",
      "0.959202106286489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.969950951589888,\n",
       " 'Accuracy': 0.8490415335463258,\n",
       " 'F1-Score': 0.8311596051541861,\n",
       " 'Class African Ancestry Precision': 0.9936102236421726,\n",
       " 'Class African Ancestry Recall': 0.9395770392749244,\n",
       " 'Class African Ancestry F1-Score': 0.9658385093167702,\n",
       " 'Class American Ancestry Precision': 0.7428571428571429,\n",
       " 'Class American Ancestry Recall': 0.30057803468208094,\n",
       " 'Class American Ancestry F1-Score': 0.4279835390946502,\n",
       " 'Class East Asian Ancestry Precision': 0.82,\n",
       " 'Class East Asian Ancestry Recall': 0.9761904761904762,\n",
       " 'Class East Asian Ancestry F1-Score': 0.8913043478260869,\n",
       " 'Class European Ancestry Precision': 0.7830188679245284,\n",
       " 'Class European Ancestry Recall': 0.9880952380952381,\n",
       " 'Class European Ancestry F1-Score': 0.8736842105263158,\n",
       " 'Class South Asian Ancestry Precision': 0.8167330677290837,\n",
       " 'Class South Asian Ancestry Recall': 0.8401639344262295,\n",
       " 'Class South Asian Ancestry F1-Score': 0.8282828282828283}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "# Train an MLP model on each version of the training data\n",
    "for X_train_now, X_test_now in zip(X_train_all_dict.values(), X_test_all_dict.values()):\n",
    "    model = RandomForestClassifier(random_state=_SEED)\n",
    "    model.fit(X_train_now, y_train_num)\n",
    "    models.append(model)\n",
    "    lr_pred_probs_train = model.predict_proba(X_train_now)\n",
    "    lr_metrics_train = utils.evaluate(lr_pred_probs_train, y_train_num, multiclass=multiclass, class_level_analysis=True, label_list=label_list)\n",
    "    lr_pred_probs_test = model.predict_proba(X_test_now)\n",
    "    lr_metrics_test = utils.evaluate(lr_pred_probs_test, y_test_num, multiclass=multiclass, class_level_analysis=True, label_list=label_list)\n",
    "    print(\"num of features: \", X_train_now.shape[1])\n",
    "    print(lr_metrics_test['AUC'])\n",
    "\n",
    "# Initialize arrays to store ensemble predictions\n",
    "ensemble_pred_probs_train = np.zeros((X_train_all_dict[0].shape[0], len(label_list)))\n",
    "ensemble_pred_probs_test = np.zeros((X_test_all_dict[0].shape[0], len(label_list)))\n",
    "\n",
    "# Predict probabilities for training and test sets using each model and combine them\n",
    "for i, (X_train_now, X_test_now) in enumerate(zip(X_train_all_dict.values(), X_test_all_dict.values())):\n",
    "    ensemble_pred_probs_train += models[i].predict_proba(X_train_now)\n",
    "    ensemble_pred_probs_test += models[i].predict_proba(X_test_now)\n",
    "    \n",
    "\n",
    "# Average the probabilities\n",
    "ensemble_pred_probs_train /= len(X_train_all_dict)\n",
    "ensemble_pred_probs_test /= len(X_test_all_dict)\n",
    "\n",
    "# Evaluate the ensemble predictions\n",
    "ensemble_metrics_train = utils.evaluate(\n",
    "    ensemble_pred_probs_train, \n",
    "    y_train_num, \n",
    "    multiclass=multiclass, \n",
    "    class_level_analysis=True, \n",
    "    label_list=label_list\n",
    ")\n",
    "\n",
    "ensemble_metrics_test = utils.evaluate(\n",
    "    ensemble_pred_probs_test, \n",
    "    y_test_num, \n",
    "    multiclass=multiclass, \n",
    "    class_level_analysis=True, \n",
    "    label_list=label_list\n",
    ")\n",
    "\n",
    "# Output the test metrics\n",
    "ensemble_metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "207affd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plots/ensemble_tree.pdf'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = model.estimators_[4]\n",
    "\n",
    "# Export the tree to Graphviz format\n",
    "dot_data = export_graphviz(estimator, out_file=None,\n",
    "                           class_names=label_list,\n",
    "                           filled=True, rounded=True,\n",
    "                           special_characters=True)\n",
    "\n",
    "# Visualize the tree using graphviz\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(\"plots/ensemble_tree\")  # Save the tree as a file\n",
    "graph.view()  # View the tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinfo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
